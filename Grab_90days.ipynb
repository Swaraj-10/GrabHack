{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuEdU5zTotfU"
      },
      "source": [
        "STEP 1 — install stable libs (run once, then restart session)\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h36Ji66gVS0",
        "outputId": "cc2a7f0d-7a06-463b-974f-fd6d8eb12638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 1A — install a stable stack (no NumPy 2.x conflicts)\n",
        "!pip -q install --force-reinstall --no-deps \\\n",
        "  \"numpy==1.26.4\" \"pandas==2.2.2\" \"pyarrow==14.0.2\" \"fastparquet==2024.5.0\" \\\n",
        "  \"scikit-learn==1.5.1\" \"lightgbm==4.3.0\" \"xgboost==2.1.1\" \\\n",
        "  \"networkx==3.3\" \"gensim==4.3.3\" \"node2vec==0.4.6\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVSQR9GpTPU_"
      },
      "source": [
        "*before running the seccond step kindly restart *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YazIGEf6ozTj"
      },
      "source": [
        "STEP 1C — Verify imports and versions (single cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_c3o7Kg4BaN"
      },
      "source": [
        "STEP 2 — Pick your parquet files (no paths to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7loE8WWfiY6F",
        "outputId": "86422a59-7e0f-4f96-9d05-17c1538e48ad"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "empty_like method already has a different docstring",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1724498694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# STEP 1C — verify imports/versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .compat import (PANDAS_INSTALLED, PYARROW_INSTALLED, arrow_cffi, arrow_is_floating, arrow_is_integer, concat,\n\u001b[0m\u001b[1;32m     22\u001b[0m                      \u001b[0mdt_DataTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_Array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_chunked_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_ChunkedArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_Table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                      pd_CategoricalDtype, pd_DataFrame, pd_Series)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdask_Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_delayed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdask_array_from_delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_delayed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdask_bag_from_delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# including the internal frames, which warnings ignores).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch_warning_stacklevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dask/array/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shuffle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0matop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# including the internal frames, which warnings ignores).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch_warning_stacklevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dask/array/backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from dask.array.dispatch import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mconcatenate_lookup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# including the internal frames, which warnings ignores).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch_warning_stacklevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dask/array/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xarray\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dask/_compatibility.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, min_version, errors)\u001b[0m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroupers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtutorial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufuncs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from xarray.backends.api import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mload_dataarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/coders.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCFDatetimeCoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFTimedeltaCoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CFDatetimeCoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CFTimedeltaCoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutOfBoundsTimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from xarray.coding.common import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mSerializationWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mVariableCoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/coding/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_chunked_array_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mduck_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinate_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoordinateTransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnputils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVIndexAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/core/duck_array_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask_array_compat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdask_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_array_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnputils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/compat/dask_array_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask_array_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreshape_blockwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnputils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/core/nputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     from numpy.core.multiarray import (  # type: ignore[attr-defined,no-redef,unused-ignore]\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/overrides.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(dispatcher)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/overrides.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(implementation)\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: empty_like method already has a different docstring"
          ]
        }
      ],
      "source": [
        "# STEP 1C — verify imports/versions\n",
        "import numpy as np, pandas as pd, pyarrow as pa\n",
        "import sklearn, lightgbm as lgb, xgboost as xgb\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "print(\"numpy        :\", np.__version__)     # expect 1.26.4\n",
        "print(\"pandas       :\", pd.__version__)     # expect 2.2.2\n",
        "print(\"pyarrow      :\", pa.__version__)     # expect 14.0.2\n",
        "print(\"scikit-learn :\", sklearn.__version__)# ~1.5.1\n",
        "print(\"lightgbm     :\", lgb.__version__)    # ~4.3.0\n",
        "print(\"xgboost      :\", xgb.__version__)    # ~2.1.x\n",
        "print(\"networkx     :\", nx.__version__)     # 3.3\n",
        "print(\"node2vec     : ok\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RB2jhOHs3_DY"
      },
      "outputs": [],
      "source": [
        "# STEP 2: upload once via picker → store with stable names in /content\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import shutil, os\n",
        "\n",
        "MERGED_DST = Path(\"/content/merged_trip_details_0.parquet\")\n",
        "TXN_DST    = Path(\"/content/transaction-0.parquet\")\n",
        "\n",
        "def detect_role(fname):\n",
        "    # Try schema first; fall back to filename keywords\n",
        "    try:\n",
        "        import pyarrow.parquet as pq\n",
        "        names = set(pq.ParquetFile(fname).schema.names)\n",
        "        if {\"trip_id\",\"trip_start_time\",\"fare_amount\"} <= names: return \"merged\"\n",
        "        if {\"transaction_id\",\"transaction_amount\",\"merchant_id\"} <= names: return \"txn\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    low = os.path.basename(fname).lower()\n",
        "    if \"merged\" in low: return \"merged\"\n",
        "    if \"transact\" in low or \"txn\" in low: return \"txn\"\n",
        "    return \"unknown\"\n",
        "\n",
        "missing = []\n",
        "if not MERGED_DST.exists(): missing.append(\"merged\")\n",
        "if not TXN_DST.exists():    missing.append(\"txn\")\n",
        "\n",
        "if not missing:\n",
        "    print(\"✅ Found both parquet files in /content/. Reusing them for all next steps.\")\n",
        "    print(\"MERGED_PATH:\", MERGED_DST)\n",
        "    print(\"TXN_PATH   :\", TXN_DST)\n",
        "else:\n",
        "    print(\"Missing:\", missing, \"→ pick your parquet file(s) in the dialog.\")\n",
        "    uploaded = files.upload()\n",
        "    leftovers = []\n",
        "    for fname in uploaded.keys():\n",
        "        role = detect_role(fname)\n",
        "        if role == \"merged\" and not MERGED_DST.exists():\n",
        "            shutil.move(fname, MERGED_DST.as_posix()); print(\"✔\", fname, \"→\", MERGED_DST)\n",
        "        elif role == \"txn\" and not TXN_DST.exists():\n",
        "            shutil.move(fname, TXN_DST.as_posix());    print(\"✔\", fname, \"→\", TXN_DST)\n",
        "        else:\n",
        "            leftovers.append(fname); print(\"ℹ Skipped\", fname, \"(role:\", role, \")\")\n",
        "    # If one role still empty and exactly one leftover exists, assign it\n",
        "    if (not MERGED_DST.exists() or not TXN_DST.exists()) and leftovers:\n",
        "        target = MERGED_DST if not MERGED_DST.exists() else TXN_DST\n",
        "        lf = leftovers[0]\n",
        "        shutil.move(lf, target.as_posix())\n",
        "        print(f\"↪ Assigned remaining {lf} → {target}\")\n",
        "\n",
        "    assert MERGED_DST.exists(), \"merged_trip_details_0.parquet still missing.\"\n",
        "    assert TXN_DST.exists(),    \"transaction-0.parquet still missing.\"\n",
        "    print(\"\\n✅ Ready. Stored at:\")\n",
        "    print(\"MERGED_PATH:\", MERGED_DST)\n",
        "    print(\"TXN_PATH   :\", TXN_DST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN0i10iTzcaZ"
      },
      "source": [
        "✅ Now run: STEP 3 (OPTIMIZED) — compact per-user transaction features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O2nRBHpbpRJ3"
      },
      "outputs": [],
      "source": [
        "# STEP 3: build compact per-user txn features (low RAM)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "TXN_PATH = Path(\"/content/transaction-0.parquet\")\n",
        "assert TXN_PATH.exists(), f\"Missing {TXN_PATH}. Run STEP 2 first.\"\n",
        "\n",
        "# 1) Load only needed columns\n",
        "use_cols = [\n",
        "    \"user_id\", \"transaction_amount\",\n",
        "    \"payment_method\", \"transaction_status\", \"merchant_category\",\n",
        "    \"transaction_date\", \"transaction_time\"\n",
        "]\n",
        "txn = pd.read_parquet(TXN_PATH, engine=\"pyarrow\", columns=use_cols)\n",
        "\n",
        "# 2) Lean dtypes + timestamp\n",
        "txn[\"user_id\"]            = txn[\"user_id\"].astype(\"string\")\n",
        "txn[\"payment_method\"]     = txn[\"payment_method\"].astype(\"string\")\n",
        "txn[\"transaction_status\"] = txn[\"transaction_status\"].astype(\"string\")\n",
        "txn[\"merchant_category\"]  = txn[\"merchant_category\"].astype(\"string\")\n",
        "txn[\"transaction_amount\"] = pd.to_numeric(txn[\"transaction_amount\"], errors=\"coerce\").astype(\"float32\")\n",
        "txn[\"txn_ts\"] = pd.to_datetime(\n",
        "    txn[\"transaction_date\"].astype(\"string\") + \" \" + txn[\"transaction_time\"].astype(\"string\"),\n",
        "    errors=\"coerce\"\n",
        ")\n",
        "\n",
        "# 3) Numeric aggregations\n",
        "g = txn.groupby(\"user_id\", observed=True)\n",
        "num_agg = g[\"transaction_amount\"].agg(\n",
        "    txn_count=\"size\",\n",
        "    txn_sum=\"sum\",\n",
        "    txn_mean=\"mean\",\n",
        "    txn_std=\"std\",\n",
        "    txn_min=\"min\",\n",
        "    txn_max=\"max\",\n",
        ").fillna(0.0).reset_index()\n",
        "\n",
        "num_agg = num_agg.astype({\n",
        "    \"txn_count\":\"int32\",\"txn_sum\":\"float32\",\"txn_mean\":\"float32\",\n",
        "    \"txn_std\":\"float32\",\"txn_min\":\"float32\",\"txn_max\":\"float32\"\n",
        "})\n",
        "\n",
        "# 4) Top-K fractional one-hots (keeps width small)\n",
        "def frac_topk(df, user_col, cat_col, k=6, prefix=None):\n",
        "    if cat_col not in df.columns:\n",
        "        return pd.DataFrame({user_col: df[user_col].unique()})\n",
        "    prefix = prefix or cat_col\n",
        "    top = df[cat_col].value_counts().nlargest(k).index.astype(\"string\")\n",
        "    mapped = df[[user_col, cat_col]].copy()\n",
        "    mapped[cat_col] = mapped[cat_col].astype(\"string\").where(mapped[cat_col].isin(top), \"__other__\")\n",
        "    cnts = mapped.groupby([user_col, cat_col], observed=True).size().astype(\"int32\")\n",
        "    frac = (cnts / cnts.groupby(level=0).sum()).rename(\"frac\").reset_index()\n",
        "    wide = frac.pivot(index=user_col, columns=cat_col, values=\"frac\").fillna(0.0)\n",
        "    cols = list(top) + [\"__other__\"]\n",
        "    for c in cols:\n",
        "        if c not in wide.columns: wide[c] = 0.0\n",
        "    wide = wide[cols]\n",
        "    wide.columns = [f\"{prefix}_{c}\" for c in wide.columns]\n",
        "    wide = wide.reset_index()\n",
        "    for c in wide.columns:\n",
        "        if c != user_col: wide[c] = wide[c].astype(\"float32\")\n",
        "    return wide\n",
        "\n",
        "pm_frac = frac_topk(txn, \"user_id\", \"payment_method\",     k=6, prefix=\"pm\")\n",
        "st_frac = frac_topk(txn, \"user_id\", \"transaction_status\", k=6, prefix=\"status\")\n",
        "mc_frac = frac_topk(txn, \"user_id\", \"merchant_category\",  k=6, prefix=\"mc\")\n",
        "\n",
        "# 5) Days since last transaction\n",
        "last_txn = g[\"txn_ts\"].max().reset_index().rename(columns={\"txn_ts\":\"last_txn_ts\"})\n",
        "global_max_ts = pd.to_datetime(txn[\"txn_ts\"].max())\n",
        "last_txn[\"days_since_last_txn\"] = ((global_max_ts - last_txn[\"last_txn_ts\"]).dt.days).fillna(9999).astype(\"int32\")\n",
        "last_txn = last_txn.drop(columns=[\"last_txn_ts\"])\n",
        "\n",
        "# 6) Merge all features\n",
        "user_txn_features = (\n",
        "    num_agg.merge(pm_frac, on=\"user_id\", how=\"left\")\n",
        "           .merge(st_frac, on=\"user_id\", how=\"left\")\n",
        "           .merge(mc_frac, on=\"user_id\", how=\"left\")\n",
        "           .merge(last_txn, on=\"user_id\", how=\"left\")\n",
        "           .fillna(0.0)\n",
        ")\n",
        "\n",
        "# 7) Save\n",
        "OUT_FULL   = \"/content/user_txn_features.parquet\"\n",
        "OUT_SAMPLE = \"/content/user_txn_features_sample.csv\"\n",
        "user_txn_features.to_parquet(OUT_FULL, index=False)\n",
        "user_txn_features.head(5).to_csv(OUT_SAMPLE, index=False)\n",
        "\n",
        "print(\"user_txn_features shape:\", user_txn_features.shape)\n",
        "print(\"Saved ->\", OUT_FULL, \"and\", OUT_SAMPLE)\n",
        "display(user_txn_features.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvtdsi5Oz0qd"
      },
      "source": [
        "STEP 4 → write model_table_90d.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8dh2RxKVz4GX"
      },
      "outputs": [],
      "source": [
        "# STEP 4: build per-trip table, merge user features, keep it numeric+lean (→ 90d)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "MERGED_PATH = Path(\"/content/merged_trip_details_0.parquet\")\n",
        "USERF_PATH  = Path(\"/content/user_txn_features.parquet\")\n",
        "assert MERGED_PATH.exists() and USERF_PATH.exists(), \"Run Step 2/3 first.\"\n",
        "\n",
        "need = [\n",
        "    \"trip_id\",\"user_id\",\"trip_date\",\"trip_start_time\",\"incident_flag\",\"safety_score\",\n",
        "    \"trip_duration\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"trip_rating\",\"cancellation_flag\",\n",
        "    \"day_of_week\",\"route_type\",\"payment_method\",\"currency\"\n",
        "]\n",
        "avail = pd.read_parquet(MERGED_PATH, engine=\"pyarrow\").columns\n",
        "use_cols = [c for c in need if c in avail]\n",
        "merged = pd.read_parquet(MERGED_PATH, engine=\"pyarrow\", columns=use_cols)\n",
        "\n",
        "merged[\"user_id\"] = merged[\"user_id\"].astype(\"string\")\n",
        "if \"trip_start_time\" in merged.columns:\n",
        "    merged[\"trip_ts\"] = pd.to_datetime(merged[\"trip_start_time\"], errors=\"coerce\")\n",
        "else:\n",
        "    merged[\"trip_ts\"] = pd.to_datetime(merged[\"trip_date\"], errors=\"coerce\")\n",
        "merged[\"trip_date_dt\"] = merged[\"trip_ts\"].dt.normalize()\n",
        "\n",
        "for c in [\"trip_duration\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"trip_rating\",\"safety_score\"]:\n",
        "    if c in merged.columns:\n",
        "        merged[c] = pd.to_numeric(merged[c], errors=\"coerce\").astype(\"float32\")\n",
        "\n",
        "if \"cancellation_flag\" in merged.columns:\n",
        "    merged[\"cancellation_flag\"] = pd.to_numeric(merged[\"cancellation_flag\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
        "merged[\"incident_flag\"] = (merged.get(\"incident_flag\", 0)).astype(\"int8\")\n",
        "\n",
        "for c in [\"day_of_week\",\"route_type\",\"payment_method\",\"currency\"]:\n",
        "    if c in merged.columns:\n",
        "        merged[c] = merged[c].astype(\"string\")\n",
        "\n",
        "def topk_onehot(df, col, k=6, prefix=None):\n",
        "    if col not in df.columns: return pd.DataFrame(index=df.index)\n",
        "    prefix = prefix or col\n",
        "    top = df[col].value_counts().nlargest(k).index.astype(\"string\")\n",
        "    s = df[col].where(df[col].isin(top), \"__other__\")\n",
        "    oh = pd.get_dummies(s, prefix=prefix)\n",
        "    want = [f\"{prefix}_{x}\" for x in list(top) + [\"__other__\"]]\n",
        "    return oh.reindex(columns=want, fill_value=0).astype(\"int8\")\n",
        "\n",
        "oh = pd.concat([\n",
        "    topk_onehot(merged, \"day_of_week\",     6, \"dow\"),\n",
        "    topk_onehot(merged, \"route_type\",      6, \"route\"),\n",
        "    topk_onehot(merged, \"payment_method\",  6, \"pm_trip\"),\n",
        "], axis=1)\n",
        "\n",
        "if {\"trip_duration\",\"trip_distance\"} <= set(merged.columns):\n",
        "    merged[\"speed_kmph\"] = (\n",
        "        merged[\"trip_distance\"] / (merged[\"trip_duration\"] / 60.0)\n",
        "    ).replace([np.inf, -np.inf], np.nan).clip(0, 160).astype(\"float32\")\n",
        "\n",
        "if {\"fare_amount\",\"trip_distance\"} <= set(merged.columns):\n",
        "    fk = (merged[\"fare_amount\"] / merged[\"trip_distance\"]).replace([np.inf,-np.inf], np.nan)\n",
        "    merged[\"fare_per_km\"] = fk.clip(0, fk.quantile(0.995)).astype(\"float32\")\n",
        "\n",
        "if {\"fare_amount\",\"tip_amount\"} <= set(merged.columns):\n",
        "    merged[\"tip_pct\"] = (\n",
        "        merged[\"tip_amount\"] / merged[\"fare_amount\"]\n",
        "    ).replace([np.inf,-np.inf], np.nan).clip(0, 1).astype(\"float32\")\n",
        "\n",
        "merged[\"hour\"]    = merged[\"trip_ts\"].dt.hour.astype(\"float32\")\n",
        "merged[\"weekday\"] = merged[\"trip_ts\"].dt.dayofweek.astype(\"float32\")\n",
        "\n",
        "keep_cols = [\n",
        "    \"trip_id\",\"user_id\",\"trip_date_dt\",\"incident_flag\",\n",
        "    \"trip_duration\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"trip_rating\",\n",
        "    \"cancellation_flag\",\"safety_score\",\"speed_kmph\",\"fare_per_km\",\"tip_pct\",\"hour\",\"weekday\"\n",
        "]\n",
        "keep_cols = [c for c in keep_cols if c in merged.columns]\n",
        "base = pd.concat([merged[keep_cols], oh], axis=1)\n",
        "\n",
        "userf = pd.read_parquet(USERF_PATH, engine=\"pyarrow\")\n",
        "userf[\"user_id\"] = userf[\"user_id\"].astype(\"string\")\n",
        "model_table = base.merge(userf, on=\"user_id\", how=\"left\").fillna(0)\n",
        "\n",
        "id_cols   = [\"trip_id\",\"user_id\",\"trip_date_dt\",\"incident_flag\"]\n",
        "num_cols  = [c for c in model_table.columns if c not in id_cols]\n",
        "for c in num_cols:\n",
        "    if model_table[c].dtype == \"bool\": model_table[c] = model_table[c].astype(\"int8\")\n",
        "    if model_table[c].dtype.name.startswith(\"float\"): model_table[c] = model_table[c].astype(\"float32\")\n",
        "    if model_table[c].dtype.name.startswith(\"int\"):   model_table[c] = model_table[c].astype(\"int32\")\n",
        "\n",
        "OUT_PARQ = \"/content/model_table_90d.parquet\"\n",
        "SAMPLE_CSV = \"/content/model_table_90d_sample.csv\"\n",
        "model_table.to_parquet(OUT_PARQ, index=False)\n",
        "model_table.head(5).to_csv(SAMPLE_CSV, index=False)\n",
        "\n",
        "cutoff_90d = model_table[\"trip_date_dt\"].max() - pd.Timedelta(days=90)\n",
        "print(\"model_table_90d:\", model_table.shape, \"→\", OUT_PARQ)\n",
        "print(\"date range:\", model_table[\"trip_date_dt\"].min().date(), \"→\", model_table[\"trip_date_dt\"].max().date())\n",
        "print(\"90-day cutoff (for training/val split later):\", cutoff_90d.date())\n",
        "print(\"numeric feature count:\", len(num_cols))\n",
        "display(model_table.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWP5GpDl3dmi"
      },
      "source": [
        "STEP 5 (90-day) LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DwRnINnB3hzD"
      },
      "outputs": [],
      "source": [
        "# STEP 5: 90-day LightGBM → probs → Nova Score → Decision Band\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "assert PARQ.exists(), \"Run STEP 4 (90d) first.\"\n",
        "\n",
        "df = pd.read_parquet(PARQ, engine=\"pyarrow\")\n",
        "\n",
        "# 1) Chronological 70/30 split (tz-safe)\n",
        "df[\"trip_date_dt\"] = pd.to_datetime(df[\"trip_date_dt\"], errors=\"coerce\")\n",
        "if str(df[\"trip_date_dt\"].dtype).startswith(\"datetime64[ns,\"):\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"].dt.tz_localize(None)\n",
        "else:\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"]\n",
        "\n",
        "cutoff = df[\"trip_date_naive\"].quantile(0.70)\n",
        "train_mask = df[\"trip_date_naive\"] <= cutoff\n",
        "val_mask   = df[\"trip_date_naive\"] >  cutoff\n",
        "\n",
        "# 2) Features / labels\n",
        "id_cols  = [\"trip_id\",\"user_id\",\"trip_date_dt\",\"trip_date_naive\",\"incident_flag\"]\n",
        "num_cols = [c for c in df.columns if c not in id_cols]\n",
        "X_train  = df.loc[train_mask, num_cols].astype(\"float32\")\n",
        "y_train  = df.loc[train_mask, \"incident_flag\"].astype(\"int8\")\n",
        "X_val    = df.loc[val_mask,   num_cols].astype(\"float32\")\n",
        "y_val    = df.loc[val_mask,   \"incident_flag\"].astype(\"int8\")\n",
        "\n",
        "print(f\"Train/Val sizes: {len(X_train)} / {len(X_val)} | features: {len(num_cols)}\")\n",
        "\n",
        "# Optional RAM cap\n",
        "N_MAX = 400_000\n",
        "if len(X_train) > N_MAX:\n",
        "    pos_idx = y_train[y_train == 1].index\n",
        "    neg_idx = y_train[y_train == 0].index\n",
        "    n_pos = min(len(pos_idx), N_MAX // 8)\n",
        "    n_neg = N_MAX - n_pos\n",
        "    pos_keep = np.random.choice(pos_idx, n_pos, replace=False)\n",
        "    neg_keep = np.random.choice(neg_idx, n_neg, replace=False)\n",
        "    keep = np.concatenate([pos_keep, neg_keep])\n",
        "    X_train = X_train.loc[keep]\n",
        "    y_train = y_train.loc[keep]\n",
        "    print(f\"Downsampled train → {len(X_train)} rows (pos={int(y_train.sum())})\")\n",
        "\n",
        "# 3) LightGBM params (class imbalance aware)\n",
        "pos_rate = max(1e-6, y_train.mean())\n",
        "scale_pos_weight = (1 - pos_rate) / pos_rate\n",
        "print(\"Pos rate (train):\", float(pos_rate), \"| scale_pos_weight:\", float(scale_pos_weight))\n",
        "\n",
        "lgb_params = dict(\n",
        "    objective=\"binary\", metric=\"auc\",\n",
        "    learning_rate=0.05, num_leaves=64, max_depth=-1, min_data_in_leaf=200,\n",
        "    feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1,\n",
        "    lambda_l1=0.0, lambda_l2=0.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    verbosity=-1,\n",
        ")\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train, feature_name=[f\"f_{i}\" for i in range(X_train.shape[1])])\n",
        "val_set   = lgb.Dataset(X_val,   label=y_val,   reference=train_set, feature_name=[f\"f_{i}\" for i in range(X_val.shape[1])])\n",
        "\n",
        "gbm = lgb.train(\n",
        "    lgb_params,\n",
        "    train_set,\n",
        "    num_boost_round=1500,\n",
        "    valid_sets=[val_set],\n",
        "    valid_names=[\"valid\"],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100, first_metric_only=True),\n",
        "               lgb.log_evaluation(period=100)],\n",
        ")\n",
        "\n",
        "# 4) Metrics\n",
        "p_val = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
        "auroc  = float(roc_auc_score(y_val, p_val))\n",
        "prauc  = float(average_precision_score(y_val, p_val))\n",
        "\n",
        "def f1_at(p, y, thr): return f1_score(y, (p >= thr).astype(int))\n",
        "grid = np.linspace(0.05, 0.50, 10)\n",
        "f1s = [f1_at(p_val, y_val, t) for t in grid]\n",
        "best_t  = float(grid[int(np.argmax(f1s))])\n",
        "best_f1 = float(max(f1s))\n",
        "print({\"AUROC\": auroc, \"PR_AUC\": prauc, \"best_F1\": best_f1, \"best_threshold\": best_t})\n",
        "\n",
        "# 5) Nova Score (sigmoid) + Decision Band\n",
        "def nova_score_from_proba(p, beta0=-3.5, beta1=10.0):\n",
        "    s = 1.0 / (1.0 + np.exp(-(beta0 - beta1 * p)))\n",
        "    return 300.0 + 600.0 * s  # ~[300,900]\n",
        "\n",
        "def decision_band(score):\n",
        "    if score < 600:    return \"<600 (Decline + coaching)\"\n",
        "    if score < 680:    return \"600–679 (Caution)\"\n",
        "    if score < 740:    return \"680–739 (Manual review)\"\n",
        "    return \"≥740 (Approve)\"\n",
        "\n",
        "val_out = df.loc[val_mask, [\"trip_id\",\"user_id\",\"incident_flag\"]].copy()\n",
        "val_out[\"proba\"] = p_val.astype(\"float32\")\n",
        "val_out[\"nova_score\"] = nova_score_from_proba(val_out[\"proba\"]).astype(\"float32\")\n",
        "val_out[\"decision_band\"] = [decision_band(s) for s in val_out[\"nova_score\"]]\n",
        "\n",
        "# 6) Per-user consolidation\n",
        "user_scores = (\n",
        "    val_out.groupby(\"user_id\", as_index=False)\n",
        "           .agg(proba_mean=(\"proba\",\"mean\"),\n",
        "                trips=(\"proba\",\"size\"))\n",
        ")\n",
        "user_scores[\"nova_score\"] = nova_score_from_proba(user_scores[\"proba_mean\"]).astype(\"float32\")\n",
        "user_scores[\"decision_band\"] = [decision_band(s) for s in user_scores[\"nova_score\"]]\n",
        "\n",
        "# 7) Save\n",
        "val_out.sample(min(10, len(val_out))).to_csv(\"/content/val_preds_90d_sample.csv\", index=False)\n",
        "user_scores.to_csv(\"/content/user_scores_90d.csv\", index=False)\n",
        "print(\"Saved -> /content/val_preds_90d_sample.csv\")\n",
        "print(\"Saved -> /content/user_scores_90d.csv\")\n",
        "\n",
        "display(user_scores.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubaOlmBz4tnm"
      },
      "source": [
        "STEP 6A — Build a small user–merchant edge list (pruned, RAM-safe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CzF9wI-w3j2S"
      },
      "outputs": [],
      "source": [
        "# STEP 6A: make a pruned bipartite edge list → /content/edges_pruned.parquet  (90d)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "TXN_PATH   = Path(\"/content/transaction-0.parquet\")\n",
        "MODEL_PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "assert TXN_PATH.exists() and MODEL_PARQ.exists(), \"Run Steps 2–5 (90d) first.\"\n",
        "\n",
        "# Load just what we need\n",
        "use_cols = [\"user_id\",\"merchant_id\"]\n",
        "txn_cols = pd.read_parquet(TXN_PATH, engine=\"pyarrow\").columns\n",
        "txn = pd.read_parquet(TXN_PATH, engine=\"pyarrow\",\n",
        "                      columns=[c for c in use_cols if c in txn_cols])\n",
        "\n",
        "txn[\"user_id\"] = txn[\"user_id\"].astype(\"string\")\n",
        "if \"merchant_id\" not in txn.columns:\n",
        "    raise RuntimeError(\"merchant_id not found in transactions.\")\n",
        "\n",
        "# Keep only users that appear in the 90d modeling table (RAM saver)\n",
        "users_90d = pd.read_parquet(MODEL_PARQ, engine=\"pyarrow\",\n",
        "                            columns=[\"user_id\"])[\"user_id\"].astype(\"string\").unique()\n",
        "txn = txn[txn[\"user_id\"].isin(set(users_90d))]\n",
        "\n",
        "# Prune to avoid huge graphs\n",
        "MIN_USER_TXN  = 5\n",
        "TOPK_PER_USER = 20\n",
        "EDGE_CAP      = 300_000  # hard cap for safety\n",
        "\n",
        "# freq per (u,m)\n",
        "pairs = (txn.groupby([\"user_id\",\"merchant_id\"], observed=True)\n",
        "           .size().rename(\"w\").reset_index())\n",
        "\n",
        "# drop low-degree users\n",
        "deg = (pairs.groupby(\"user_id\", observed=True)[\"merchant_id\"]\n",
        "            .nunique().rename(\"deg\").reset_index())\n",
        "good_users = set(deg.loc[deg[\"deg\"] >= MIN_USER_TXN, \"user_id\"])\n",
        "pairs = pairs[pairs[\"user_id\"].isin(good_users)]\n",
        "\n",
        "# top-K merchants per user\n",
        "pairs[\"rn\"] = pairs.groupby(\"user_id\")[\"w\"].rank(method=\"first\", ascending=False)\n",
        "pairs = pairs[pairs[\"rn\"] <= TOPK_PER_USER].drop(columns=\"rn\")\n",
        "\n",
        "# cap total edges\n",
        "if len(pairs) > EDGE_CAP:\n",
        "    pairs = pairs.sample(EDGE_CAP, random_state=42).reset_index(drop=True)\n",
        "\n",
        "pairs = pairs.reset_index(drop=True)\n",
        "pairs.to_parquet(\"/content/edges_pruned.parquet\", index=False)\n",
        "\n",
        "print(f\"Edges pruned: {len(pairs)}  | users: {pairs['user_id'].nunique()}  | merchants: {pairs['merchant_id'].nunique()}\")\n",
        "print(\"Saved -> /content/edges_pruned.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9WntGLW6Foi"
      },
      "source": [
        "STEP 6B — make user embeddings (Node2Vec → SVD fallback), save to /content/user_n2v.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9NOH3E2643ek"
      },
      "outputs": [],
      "source": [
        "# STEP 6B: build /content/user_n2v.parquet (Node2Vec or SVD fallback, RAM-safe)\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import coo_matrix\n",
        "import networkx as nx\n",
        "\n",
        "EDGES = Path(\"/content/edges_pruned.parquet\")\n",
        "assert EDGES.exists(), \"Run STEP 6A first.\"\n",
        "\n",
        "edges = pd.read_parquet(EDGES, engine=\"pyarrow\")\n",
        "edges[\"user_id\"]     = edges[\"user_id\"].astype(\"string\")\n",
        "edges[\"merchant_id\"] = edges[\"merchant_id\"].astype(\"string\")\n",
        "if \"w\" not in edges.columns:\n",
        "    edges[\"w\"] = 1.0\n",
        "edges[\"w\"] = edges[\"w\"].astype(\"float32\")\n",
        "\n",
        "print(f\"Edges for embedding: {len(edges)} | users={edges['user_id'].nunique()} | merchants={edges['merchant_id'].nunique()}\")\n",
        "\n",
        "EMB_DIM = 32   # small & safe for Colab Free\n",
        "method_used = None\n",
        "user_emb = None\n",
        "\n",
        "# ---- Try Node2Vec (small params) ----\n",
        "try:\n",
        "    from node2vec import Node2Vec\n",
        "\n",
        "    G = nx.Graph()\n",
        "    # prefix with U_/M_ to avoid ID collisions\n",
        "    G.add_edges_from(\n",
        "        (f\"U_{u}\", f\"M_{m}\", {\"weight\": float(w)})\n",
        "        for u, m, w in edges.itertuples(index=False)\n",
        "    )\n",
        "    print(f\"Graph: |V|={G.number_of_nodes()} |E|={G.number_of_edges()}\")\n",
        "\n",
        "    n2v = Node2Vec(\n",
        "        G,\n",
        "        dimensions=EMB_DIM,\n",
        "        walk_length=8,\n",
        "        num_walks=4,\n",
        "        p=1, q=1,\n",
        "        workers=1,                 # keep RAM/CPU low\n",
        "        weight_key=\"weight\",\n",
        "        quiet=True,\n",
        "        seed=42,\n",
        "    )\n",
        "    w2v = n2v.fit(window=5, min_count=1, batch_words=512, epochs=3)\n",
        "\n",
        "    u_nodes = [n for n in G.nodes if str(n).startswith(\"U_\")]\n",
        "    rows = []\n",
        "    zero = np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    for n in u_nodes:\n",
        "        vec = w2v.wv[n] if n in w2v.wv else zero\n",
        "        rows.append([n[2:]] + list(map(float, vec)))\n",
        "\n",
        "    cols = [\"user_id\"] + [f\"user_n2v_{i}\" for i in range(EMB_DIM)]\n",
        "    user_emb = pd.DataFrame(rows, columns=cols)\n",
        "    method_used = \"node2vec\"\n",
        "\n",
        "    del w2v, n2v, G\n",
        "    gc.collect()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Node2Vec failed (falling back to SVD):\", e)\n",
        "\n",
        "# ---- SVD fallback (very RAM-friendly) ----\n",
        "if user_emb is None:\n",
        "    u_codes = edges[\"user_id\"].astype(\"category\")\n",
        "    m_codes = edges[\"merchant_id\"].astype(\"category\")\n",
        "    uid = u_codes.cat.codes.values\n",
        "    mid = m_codes.cat.codes.values\n",
        "    w   = edges[\"w\"].values\n",
        "\n",
        "    n_u = int(uid.max()) + 1\n",
        "    n_m = int(mid.max()) + 1\n",
        "    X = coo_matrix((w, (uid, mid)), shape=(n_u, n_m), dtype=np.float32).tocsr()\n",
        "\n",
        "    svd = TruncatedSVD(n_components=EMB_DIM, n_iter=7, random_state=42)\n",
        "    U = svd.fit_transform(X).astype(\"float32\")\n",
        "\n",
        "    # map codes back to user_id strings\n",
        "    code_to_user = pd.DataFrame({\n",
        "        \"user_id\": u_codes.cat.categories.astype(\"string\"),\n",
        "        \"uid\": np.arange(len(u_codes.cat.categories))\n",
        "    }).sort_values(\"uid\").reset_index(drop=True)\n",
        "\n",
        "    user_emb = pd.DataFrame(U, columns=[f\"user_n2v_{i}\" for i in range(EMB_DIM)])\n",
        "    user_emb.insert(0, \"user_id\", code_to_user[\"user_id\"].values)\n",
        "    method_used = \"svd_fallback\"\n",
        "\n",
        "    del X, svd, U\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"Embeddings ready: {user_emb.shape}  | method={method_used}\")\n",
        "user_emb.to_parquet(\"/content/user_n2v.parquet\", index=False)\n",
        "user_emb.head(10).to_csv(\"/content/user_n2v_sample.csv\", index=False)\n",
        "print(\"Saved -> /content/user_n2v.parquet and /content/user_n2v_sample.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dXh7eRv70cO"
      },
      "source": [
        "# STEP 6C-SAFE (part 1/2): prep + TRAIN on a stratified sample without full merge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "43NS2EKT6MAI"
      },
      "outputs": [],
      "source": [
        "# STEP 6C-SAFE (part 1/2): prep + TRAIN on a stratified sample without full merge\n",
        "import pandas as pd, numpy as np, gc\n",
        "from pathlib import Path\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "EMB  = Path(\"/content/user_n2v.parquet\")\n",
        "assert PARQ.exists() and EMB.exists(), \"Missing inputs. Run Steps 4–6B first.\"\n",
        "\n",
        "# ---- 1) Load base table (no merge), get numeric columns ----\n",
        "df  = pd.read_parquet(PARQ, engine=\"pyarrow\")\n",
        "df[\"user_id\"] = df[\"user_id\"].astype(\"string\")\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "id_cols = [\"trip_id\",\"user_id\",\"trip_date_dt\",\"incident_flag\"]\n",
        "num_base = [c for c in df.columns if c not in id_cols and np.issubdtype(df[c].dtype, np.number)]\n",
        "\n",
        "# ---- 2) Split (70/30 by time) ----\n",
        "df[\"trip_date_dt\"] = pd.to_datetime(df[\"trip_date_dt\"], errors=\"coerce\")\n",
        "if str(df[\"trip_date_dt\"].dtype).startswith(\"datetime64[ns,\"):\n",
        "    trip_naive = df[\"trip_date_dt\"].dt.tz_localize(None)\n",
        "else:\n",
        "    trip_naive = df[\"trip_date_dt\"]\n",
        "\n",
        "cutoff = trip_naive.quantile(0.70)\n",
        "train_mask = trip_naive <= cutoff\n",
        "val_mask   = trip_naive >  cutoff\n",
        "\n",
        "# ---- 3) Load embeddings & index for fast mapping ----\n",
        "emb = pd.read_parquet(EMB, engine=\"pyarrow\")\n",
        "emb[\"user_id\"] = emb[\"user_id\"].astype(\"string\")\n",
        "emb = emb.set_index(\"user_id\")\n",
        "emb_cols = [c for c in emb.columns if c.startswith(\"user_n2v_\")]\n",
        "EMB_DIM = len(emb_cols)\n",
        "\n",
        "# ---- 4) Stratified downsample BEFORE building features ----\n",
        "N_MAX = 300_000  # adjust down if still tight (e.g., 200_000)\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "y = df[\"incident_flag\"].astype(\"int8\")\n",
        "pos_idx = np.where(train_mask & (y == 1))[0]\n",
        "neg_idx = np.where(train_mask & (y == 0))[0]\n",
        "\n",
        "n_pos = min(len(pos_idx), max(1, N_MAX // 8))\n",
        "n_neg = min(len(neg_idx), max(1, N_MAX - n_pos))\n",
        "keep = np.concatenate([rng.choice(pos_idx, n_pos, replace=False),\n",
        "                       rng.choice(neg_idx, n_neg, replace=False)])\n",
        "rng.shuffle(keep)\n",
        "\n",
        "print(f\"Train downsample: pos={n_pos}, neg={n_neg}, total={len(keep)}\")\n",
        "\n",
        "# ---- 5) Build TRAIN features on the kept rows ONLY (map embeds column-wise) ----\n",
        "X_train = df.loc[keep, num_base].astype(\"float32\").copy()\n",
        "uid_train = df.loc[keep, \"user_id\"]\n",
        "\n",
        "# add embeddings via vectorized map per column (no big merge)\n",
        "for c in emb_cols:\n",
        "    X_train[c] = uid_train.map(emb[c]).astype(\"float32\").fillna(0.0)\n",
        "\n",
        "y_train = y.iloc[keep].to_numpy()\n",
        "\n",
        "# ---- 6) Build a compact VALIDATION sample for live metrics (keep full scoring for 6C part 2) ----\n",
        "VAL_SAMPLE = 250_000  # small holdout for AUC; full validation scored streaming later\n",
        "val_idx_all = np.where(val_mask)[0]\n",
        "val_keep = rng.choice(val_idx_all, size=min(VAL_SAMPLE, len(val_idx_all)), replace=False)\n",
        "\n",
        "X_val_small = df.loc[val_keep, num_base].astype(\"float32\").copy()\n",
        "uid_val_small = df.loc[val_keep, \"user_id\"]\n",
        "for c in emb_cols:\n",
        "    X_val_small[c] = uid_val_small.map(emb[c]).astype(\"float32\").fillna(0.0)\n",
        "y_val_small = y.iloc[val_keep].to_numpy()\n",
        "\n",
        "# ---- 7) Train LightGBM ----\n",
        "pos_rate = float(max(1e-6, y_train.mean()))\n",
        "scale_pos_weight = (1 - pos_rate) / pos_rate\n",
        "params = dict(objective=\"binary\", metric=\"auc\",\n",
        "              learning_rate=0.05, num_leaves=64, min_data_in_leaf=200,\n",
        "              feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1,\n",
        "              scale_pos_weight=scale_pos_weight, verbosity=-1)\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train)\n",
        "val_set   = lgb.Dataset(X_val_small, label=y_val_small, reference=train_set)\n",
        "\n",
        "gbm = lgb.train(params, train_set, num_boost_round=1200,\n",
        "                valid_sets=[val_set], valid_names=[\"valid\"],\n",
        "                callbacks=[lgb.early_stopping(100, first_metric_only=True),\n",
        "                           lgb.log_evaluation(100)])\n",
        "\n",
        "# quick metrics on sampled val\n",
        "p_small = gbm.predict(X_val_small, num_iteration=gbm.best_iteration)\n",
        "auroc = float(roc_auc_score(y_val_small, p_small))\n",
        "prauc = float(average_precision_score(y_val_small, p_small))\n",
        "grid = np.linspace(0.05, 0.50, 10)\n",
        "best_t = float(grid[int(np.argmax([f1_score(y_val_small, (p_small>=t).astype(int), zero_division=0) for t in grid]))])\n",
        "print({\"AUROC_sample\": auroc, \"PR_AUC_sample\": prauc, \"best_thr_sample\": best_t})\n",
        "\n",
        "# keep objects for part 2\n",
        "val_idx_all = val_idx_all  # full validation row indices\n",
        "num_base = num_base\n",
        "emb_cols = emb_cols\n",
        "emb = emb  # indexed by user_id\n",
        "df_ids = df[[\"trip_id\",\"user_id\"]].copy()\n",
        "del X_train, X_val_small, uid_train, uid_val_small\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHdAS5OTvsp5"
      },
      "source": [
        "# STEP 6C-SAFE (part 2/2): STREAM full validation to get per-user scores (low RAM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SVyMaLNgvt_S"
      },
      "outputs": [],
      "source": [
        "# STEP 6C-SAFE (part 2/2): STREAM full validation to get per-user scores (low RAM)\n",
        "import numpy as np, pandas as pd, gc\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# --- sanity checks (must have run 6C part-1) ---\n",
        "needed = [\"df\", \"df_ids\", \"val_idx_all\", \"num_base\", \"emb\", \"emb_cols\", \"gbm\"]\n",
        "for name in needed:\n",
        "    assert name in globals(), f\"Missing '{name}'. Please run 6C part-1 first.\"\n",
        "\n",
        "# --- chunked scoring over full validation set ---\n",
        "CHUNK = 200_000  # lower if RAM is tight (e.g., 120_000)\n",
        "val_idx_all = np.asarray(val_idx_all)\n",
        "n_val = len(val_idx_all)\n",
        "print(f\"Scoring full validation in chunks: {n_val} rows\")\n",
        "\n",
        "# arrays for metrics\n",
        "y_full = df.loc[val_idx_all, \"incident_flag\"].astype(\"int8\").to_numpy()\n",
        "p_full = np.empty(n_val, dtype=np.float32)\n",
        "\n",
        "# per-user aggregation\n",
        "from collections import defaultdict\n",
        "sum_d = defaultdict(float)\n",
        "cnt_d = defaultdict(int)\n",
        "\n",
        "sample_rows = []  # keep a tiny preview (up to 10 rows)\n",
        "\n",
        "for a in range(0, n_val, CHUNK):\n",
        "    b = min(n_val, a + CHUNK)\n",
        "    idx = val_idx_all[a:b]\n",
        "\n",
        "    # base numeric block\n",
        "    Xb = df.loc[idx, num_base].astype(\"float32\").copy()\n",
        "    # user ids for this chunk\n",
        "    uids = df.loc[idx, \"user_id\"]\n",
        "\n",
        "    # add embeddings per column via fast map; fill missing with 0.0\n",
        "    for c in emb_cols:\n",
        "        Xb[c] = uids.map(emb[c]).astype(\"float32\").fillna(0.0)\n",
        "\n",
        "    # predict\n",
        "    pb = gbm.predict(Xb, num_iteration=gbm.best_iteration).astype(\"float32\")\n",
        "    p_full[a:b] = pb\n",
        "\n",
        "    # aggregate per user\n",
        "    uids_arr = uids.astype(\"string\").values\n",
        "    for uid, pr in zip(uids_arr, pb):\n",
        "        sum_d[uid] += float(pr)\n",
        "        cnt_d[uid] += 1\n",
        "\n",
        "    # collect a tiny preview of per-trip predictions\n",
        "    if len(sample_rows) < 10:\n",
        "        trips = df_ids.loc[idx, \"trip_id\"].astype(\"string\").values\n",
        "        need = 10 - len(sample_rows)\n",
        "        sample_rows.extend(list(zip(trips[:need], uids_arr[:need], pb[:need])))\n",
        "\n",
        "    # cleanup\n",
        "    del Xb, uids, uids_arr, pb\n",
        "    gc.collect()\n",
        "\n",
        "# --- metrics on full validation ---\n",
        "auroc  = float(roc_auc_score(y_full, p_full))\n",
        "prauc  = float(average_precision_score(y_full, p_full))\n",
        "print({\"AUROC_full\": auroc, \"PR_AUC_full\": prauc})\n",
        "\n",
        "# --- per-user table (fixed dtype handling) ---\n",
        "users = np.array(list(sum_d.keys()), dtype=object)           # keep as object; not numpy \"string\"\n",
        "proba_mean = np.array([sum_d[u] / cnt_d[u] for u in users], dtype=np.float32)\n",
        "trips = np.array([cnt_d[u] for u in users], dtype=np.int32)\n",
        "\n",
        "user_scores = pd.DataFrame({\n",
        "    \"user_id\": users,        # object now; cast to pandas string below\n",
        "    \"proba_mean\": proba_mean,\n",
        "    \"trips\": trips\n",
        "})\n",
        "user_scores[\"user_id\"] = user_scores[\"user_id\"].astype(\"string\")\n",
        "\n",
        "def nova_score_from_proba(p, beta0=-3.5, beta1=10.0):\n",
        "    s = 1.0 / (1.0 + np.exp(-(beta0 - beta1 * p)))\n",
        "    return 300.0 + 600.0 * s\n",
        "\n",
        "def decision_band(s):\n",
        "    if s < 600:  return \"<600 (Decline + coaching)\"\n",
        "    if s < 680:  return \"600–679 (Caution)\"\n",
        "    if s < 740:  return \"680–739 (Manual review)\"\n",
        "    return \"≥740 (Approve)\"\n",
        "\n",
        "user_scores[\"nova_score\"] = nova_score_from_proba(user_scores[\"proba_mean\"]).astype(\"float32\")\n",
        "user_scores[\"decision_band\"] = [decision_band(s) for s in user_scores[\"nova_score\"]]\n",
        "\n",
        "# --- save outputs ---\n",
        "user_scores.to_csv(\"/content/user_scores_90d_n2v.csv\", index=False)\n",
        "\n",
        "if sample_rows:\n",
        "    samp = pd.DataFrame(sample_rows, columns=[\"trip_id\",\"user_id\",\"proba\"])\n",
        "    samp[\"nova_score\"] = nova_score_from_proba(samp[\"proba\"]).astype(\"float32\")\n",
        "    samp[\"decision_band\"] = [decision_band(s) for s in samp[\"nova_score\"]]\n",
        "    samp.to_csv(\"/content/val_preds_90d_n2v_sample.csv\", index=False)\n",
        "\n",
        "print(\"Saved -> /content/user_scores_90d_n2v.csv\")\n",
        "print(\"Saved -> /content/val_preds_90d_n2v_sample.csv\")\n",
        "\n",
        "display(user_scores.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EexYN9l9Akv"
      },
      "source": [
        "# STEP 6D — Calibrate Nova Score with a well-spread sigmoid, then banding (90d)\n",
        "*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QgHr36pD7wuh"
      },
      "outputs": [],
      "source": [
        "# STEP 6D — Calibrate Nova Score with a well-spread sigmoid, then banding (90d)\n",
        "# Reads:  /content/user_scores_90d_n2v.csv\n",
        "# Writes: /content/user_scores_90d_n2v_cal.csv\n",
        "#         /content/val_preds_90d_n2v_cal_sample.csv  (if preview file exists)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "USERS_PATH = \"/content/user_scores_90d_n2v.csv\"\n",
        "assert os.path.exists(USERS_PATH), \"Run STEP 6C first to create user_scores_90d_n2v.csv.\"\n",
        "\n",
        "def nova_score_sigmoid(p, k=3.0, pivot=0.5):\n",
        "    # s in [0,1], score = 300 + 600*s\n",
        "    s = 1.0 / (1.0 + np.exp(-k * (pivot - p)))\n",
        "    return 300.0 + 600.0 * s\n",
        "\n",
        "def decision_band_v2(s):\n",
        "    if s >= 800: return \"≥800 (Auto-approve, large limit)\"\n",
        "    if s >= 700: return \"700–799 (Standard approve)\"\n",
        "    if s >= 600: return \"600–699 (Manual review)\"\n",
        "    return \"<600 (Decline + coaching)\"\n",
        "\n",
        "# ---- 1) Calibrate per-user outputs ----\n",
        "users = pd.read_csv(USERS_PATH, dtype={\"user_id\":\"string\"})\n",
        "if \"proba_mean\" not in users.columns:\n",
        "    raise ValueError(\"Expected column 'proba_mean' not found in user scores.\")\n",
        "\n",
        "users[\"nova_score\"] = nova_score_sigmoid(users[\"proba_mean\"].astype(\"float32\")).astype(\"float32\")\n",
        "users[\"decision_band\"] = [decision_band_v2(s) for s in users[\"nova_score\"]]\n",
        "\n",
        "OUT_USERS = \"/content/user_scores_90d_n2v_cal.csv\"\n",
        "users.to_csv(OUT_USERS, index=False)\n",
        "print(f\"Saved → {OUT_USERS}\")\n",
        "print(users.head(10).to_string(index=False))\n",
        "\n",
        "# ---- 2) (Optional) Calibrate tiny per-trip preview if it exists ----\n",
        "SAMPLE_IN  = \"/content/val_preds_90d_n2v_sample.csv\"\n",
        "SAMPLE_OUT = \"/content/val_preds_90d_n2v_cal_sample.csv\"\n",
        "if os.path.exists(SAMPLE_IN):\n",
        "    samp = pd.read_csv(SAMPLE_IN)\n",
        "    if \"proba\" in samp.columns:\n",
        "        samp[\"nova_score\"] = nova_score_sigmoid(samp[\"proba\"].astype(\"float32\")).astype(\"float32\")\n",
        "        samp[\"decision_band\"] = [decision_band_v2(s) for s in samp[\"nova_score\"]]\n",
        "        samp.to_csv(SAMPLE_OUT, index=False)\n",
        "        print(f\"Saved → {SAMPLE_OUT}\")\n",
        "    else:\n",
        "        print(f\"Preview file found but missing 'proba' column: {SAMPLE_IN}\")\n",
        "else:\n",
        "    print(\"No per-trip preview found to calibrate (this is fine).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3cHXrbLjF5m"
      },
      "source": [
        "# STEP 7A-PREP (90d): Merge embeds, select numeric features, standardize, split, and SAVE to disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JIrGu_NE9IsT"
      },
      "outputs": [],
      "source": [
        "# STEP 7A-PREP (90d): Merge embeds, select numeric features, standardize, split, and SAVE to disk.\n",
        "import numpy as np, pandas as pd, json, gc\n",
        "from pathlib import Path\n",
        "\n",
        "PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "EMB  = Path(\"/content/user_n2v.parquet\")\n",
        "assert PARQ.exists() and EMB.exists(), \"Missing inputs. Make sure Steps 4–6B ran.\"\n",
        "\n",
        "outdir = Path(\"/content/ft90\"); outdir.mkdir(exist_ok=True)\n",
        "\n",
        "# 1) Load + merge once\n",
        "df  = pd.read_parquet(PARQ, engine=\"pyarrow\")\n",
        "emb = pd.read_parquet(EMB,  engine=\"pyarrow\")\n",
        "\n",
        "df[\"user_id\"]  = df[\"user_id\"].astype(\"string\")\n",
        "emb[\"user_id\"] = emb[\"user_id\"].astype(\"string\")\n",
        "df = df.merge(emb, on=\"user_id\", how=\"left\").fillna(0.0)\n",
        "\n",
        "# 2) Chronological 70/30 split\n",
        "df[\"trip_date_dt\"] = pd.to_datetime(df[\"trip_date_dt\"], errors=\"coerce\")\n",
        "if str(df[\"trip_date_dt\"].dtype).startswith(\"datetime64[ns,\"):\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"].dt.tz_localize(None)\n",
        "else:\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"]\n",
        "\n",
        "cutoff = df[\"trip_date_naive\"].quantile(0.70)\n",
        "train_mask = df[\"trip_date_naive\"] <= cutoff\n",
        "val_mask   = df[\"trip_date_naive\"] >  cutoff\n",
        "\n",
        "# 3) Numeric-only features\n",
        "id_cols = [\"trip_id\",\"user_id\",\"trip_date_dt\",\"trip_date_naive\",\"incident_flag\"]\n",
        "num_cols = [c for c in df.columns if c not in id_cols and np.issubdtype(df[c].dtype, np.number)]\n",
        "with open(outdir/\"num_cols.json\",\"w\") as f:\n",
        "    json.dump(num_cols, f)\n",
        "\n",
        "X_train_df = df.loc[train_mask, num_cols].astype(\"float32\")\n",
        "y_train = df.loc[train_mask, \"incident_flag\"].astype(\"int8\").to_numpy()\n",
        "X_val_df = df.loc[val_mask,   num_cols].astype(\"float32\")\n",
        "y_val   = df.loc[val_mask,   \"incident_flag\"].astype(\"int8\").to_numpy()\n",
        "\n",
        "# 4) Standardize (fit on train only), save mu/sd for reuse\n",
        "mu = X_train_df.mean(axis=0).to_numpy(dtype=\"float32\")\n",
        "sd = X_train_df.std(axis=0).replace(0, 1.0).to_numpy(dtype=\"float32\")\n",
        "\n",
        "X_train = (X_train_df.to_numpy() - mu) / sd\n",
        "X_val   = (X_val_df.to_numpy()   - mu) / sd\n",
        "\n",
        "# 5) Persist arrays (will be memory-mapped later)\n",
        "np.save(outdir/\"X_train.npy\", X_train)\n",
        "np.save(outdir/\"y_train.npy\", y_train)\n",
        "np.save(outdir/\"X_val.npy\",   X_val)\n",
        "np.save(outdir/\"y_val.npy\",   y_val)\n",
        "np.save(outdir/\"mu.npy\",      mu)\n",
        "np.save(outdir/\"sd.npy\",      sd)\n",
        "\n",
        "# Also persist identifiers (aligned to X_val order) for export step\n",
        "val_ids = df.loc[val_mask, [\"trip_id\",\"user_id\",\"incident_flag\"]].reset_index(drop=True)\n",
        "val_ids.to_parquet(outdir/\"val_ids.parquet\", index=False)\n",
        "\n",
        "# Free RAM\n",
        "del df, emb, X_train_df, X_val_df, X_train, X_val, y_train, y_val, val_ids\n",
        "gc.collect()\n",
        "print(\"PREP DONE → saved arrays in /content/ft90\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TeZWbeEyo_l"
      },
      "source": [
        "# STEP 7A-SPLIT-1 (90d): Create small shards from the big training arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QbSx_ZUEjItf"
      },
      "outputs": [],
      "source": [
        "# STEP 7A-SPLIT-1 (90d): Create small shards from the big training arrays.\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "ftdir = Path(\"/content/ft90\")\n",
        "X_tr = np.load(ftdir/\"X_train.npy\", mmap_mode=\"r\")\n",
        "y_tr = np.load(ftdir/\"y_train.npy\", mmap_mode=\"r\")\n",
        "\n",
        "# tune shard_rows downward if RAM is tight (e.g., 20000)\n",
        "shard_rows = 50000\n",
        "ft_shards = ftdir/\"shards\"\n",
        "ft_shards.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "n, f = X_tr.shape\n",
        "n_shards = (n + shard_rows - 1)//shard_rows\n",
        "print(f\"Train shape: {X_tr.shape}  -> writing {n_shards} shards (~{shard_rows} rows each)\")\n",
        "\n",
        "for si in range(n_shards):\n",
        "    a = si*shard_rows\n",
        "    b = min(n, (si+1)*shard_rows)\n",
        "    Xp = np.asarray(X_tr[a:b], dtype=np.float32, order=\"C\")  # compact\n",
        "    yp = np.asarray(y_tr[a:b], dtype=np.float32, order=\"C\")\n",
        "    np.save(ft_shards/f\"X_{si:03d}.npy\", Xp)\n",
        "    np.save(ft_shards/f\"y_{si:03d}.npy\", yp)\n",
        "    print(f\"  wrote shard {si+1}/{n_shards}: rows {a}:{b}\")\n",
        "\n",
        "print(\"DONE → shards at /content/ft90/shards\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPyeLJoZW3sL"
      },
      "source": [
        "# STEP 7A-SPLIT-2 (90d): Re-split existing shards into smaller *micro-shards* to reduce RAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vmho0QFvysaE"
      },
      "outputs": [],
      "source": [
        "# STEP 7A-SPLIT-2 (90d): Re-split existing shards into smaller *micro-shards* to reduce RAM\n",
        "import numpy as np, os, glob\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/ft90\")\n",
        "IN_DIR  = BASE / \"shards\"        # from 7A-SPLIT-1\n",
        "OUT_DIR = BASE / \"micro_shards\"  # new, smaller shards live here\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# tune this down if you still see OOM (e.g., 5000)\n",
        "MICRO_ROWS = 10_000\n",
        "\n",
        "in_X = sorted(glob.glob(str(IN_DIR/\"X_*.npy\")))\n",
        "in_y = sorted(glob.glob(str(IN_DIR/\"y_*.npy\")))\n",
        "assert in_X and in_y and len(in_X) == len(in_y), \"Run 7A-SPLIT-1 first to create shards.\"\n",
        "\n",
        "print(f\"Re-splitting {len(in_X)} shards into micro-shards of {MICRO_ROWS} rows each…\")\n",
        "\n",
        "ms_written = 0\n",
        "for Xp_path, yp_path in zip(in_X, in_y):\n",
        "    Xp = np.load(Xp_path, mmap_mode=\"r\")\n",
        "    yp = np.load(yp_path, mmap_mode=\"r\")\n",
        "\n",
        "    n = Xp.shape[0]\n",
        "    # stem like \"X_003\" -> base tag \"003\"\n",
        "    tag = Path(Xp_path).stem.split(\"X_\")[1]\n",
        "\n",
        "    for a in range(0, n, MICRO_ROWS):\n",
        "        b = min(n, a + MICRO_ROWS)\n",
        "        Xm = np.asarray(Xp[a:b], dtype=np.float32, order=\"C\")\n",
        "        ym = np.asarray(yp[a:b], dtype=np.float32, order=\"C\")\n",
        "        # name: X_003_000.npy , y_003_000.npy , etc.\n",
        "        piece = f\"{int(a/MICRO_ROWS):03d}\"\n",
        "        np.save(OUT_DIR/f\"X_{tag}_{piece}.npy\", Xm)\n",
        "        np.save(OUT_DIR/f\"y_{tag}_{piece}.npy\", ym)\n",
        "        ms_written += 1\n",
        "\n",
        "print(f\"Done. Wrote {ms_written} micro-shards to {OUT_DIR}\")\n",
        "\n",
        "# (Optional) delete the original larger shards to free space\n",
        "DELETE_OLD = True\n",
        "if DELETE_OLD:\n",
        "    for p in in_X + in_y:\n",
        "        try: os.remove(p)\n",
        "        except Exception: pass\n",
        "    print(\"Removed original shards in\", IN_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRhIryeDrvkU"
      },
      "source": [
        "# STEP 7A-TRAIN-MICRO (90d): Train FT-Transformer using micro-shards (extra RAM-safe) -- split 2 -- part 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LXcgU_izW8QS"
      },
      "outputs": [],
      "source": [
        "# STEP 7A-TRAIN-MICRO (90d): Train FT-Transformer using micro-shards (extra RAM-safe)\n",
        "import numpy as np, time, json, glob\n",
        "from pathlib import Path\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ftdir   = Path(\"/content/ft90\")\n",
        "shards  = ftdir / \"micro_shards\"   # <<< use micro-shards\n",
        "with open(ftdir/\"num_cols.json\") as f: num_cols = json.load(f)\n",
        "\n",
        "# --- Validation memmap (small CHUNK to be safe) ---\n",
        "X_val = np.load(ftdir/\"X_val.npy\", mmap_mode=\"r\")\n",
        "y_val = np.load(ftdir/\"y_val.npy\")\n",
        "print(f\"Val shape: {X_val.shape}\")\n",
        "\n",
        "# ---- Iterable dataset over micro-shards ----\n",
        "class ShardBatcher(IterableDataset):\n",
        "    def __init__(self, shard_dir: Path, batch_size=96, shuffle=True, max_shards=None):\n",
        "        self.dir = shard_dir\n",
        "        self.batch = int(batch_size)\n",
        "        self.shuffle = shuffle\n",
        "        self.files_X = sorted([p for p in self.dir.glob(\"X_*.npy\")])\n",
        "        if not self.files_X:\n",
        "            raise FileNotFoundError(f\"No micro-shard files in {self.dir}. Run 7A-SPLIT-2 first.\")\n",
        "        if max_shards is not None:\n",
        "            self.files_X = self.files_X[:int(max_shards)]\n",
        "        print(f\"ShardBatcher: {len(self.files_X)} micro-shards, batch={self.batch}, shuffle={self.shuffle}\")\n",
        "    def __iter__(self):\n",
        "        rng = np.random.default_rng(42)\n",
        "        shard_paths = self.files_X.copy()\n",
        "        if self.shuffle: rng.shuffle(shard_paths)\n",
        "        total = len(shard_paths)\n",
        "        for si, Xp_path in enumerate(shard_paths):\n",
        "            # Pair y by replacing prefix\n",
        "            yp_path = Path(str(Xp_path).replace(\"/X_\", \"/y_\").replace(\"\\\\X_\", \"\\\\y_\"))\n",
        "            Xp = np.load(Xp_path, mmap_mode=\"r\")\n",
        "            yp = np.load(yp_path, mmap_mode=\"r\")\n",
        "            n = Xp.shape[0]\n",
        "            order = np.arange(n)\n",
        "            if self.shuffle: rng.shuffle(order)\n",
        "            print(f\"→ micro {si+1}/{total}: {Xp_path.name}  rows={n}\")\n",
        "            for s in range(0, n, self.batch):\n",
        "                b = order[s:s+self.batch]\n",
        "                xb = np.asarray(Xp[b], dtype=np.float32, order=\"C\")\n",
        "                yb = np.asarray(yp[b], dtype=np.float32, order=\"C\")\n",
        "                yield torch.from_numpy(xb), torch.from_numpy(yb)\n",
        "\n",
        "# ---- Speed/RAM knobs (smaller than before) ----\n",
        "BATCH = 96            # smaller batches\n",
        "ACCUM = 4             # effective batch = 96*4\n",
        "MAX_BATCHES_PER_EPOCH = 450   # tighter budget\n",
        "EPOCHS, patience = 2, 1\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ShardBatcher(shards, batch_size=BATCH, shuffle=True),\n",
        "    batch_size=None, num_workers=0, pin_memory=False\n",
        ")\n",
        "\n",
        "# ---- Compact FT-Transformer ----\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, n_features, d_model=48, n_layers=2, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(n_features, d_model)*0.02)\n",
        "        self.bias   = nn.Parameter(torch.zeros(n_features, d_model))\n",
        "        self.cls    = nn.Parameter(torch.zeros(1,1,d_model))\n",
        "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
        "                                         dim_feedforward=4*d_model,\n",
        "                                         dropout=dropout, batch_first=True,\n",
        "                                         activation=\"gelu\")\n",
        "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, 1)\n",
        "    def forward(self, x):\n",
        "        t = x.unsqueeze(-1)*self.weight + self.bias\n",
        "        z = torch.cat([self.cls.expand(t.size(0),1,-1), t], dim=1)\n",
        "        z = self.encoder(z)\n",
        "        z = self.norm(z[:,0])\n",
        "        return self.head(z).squeeze(-1)\n",
        "\n",
        "# class weight from first micro-shard\n",
        "y_files = sorted(glob.glob(str(shards/\"y_*.npy\")))\n",
        "first_y = np.load(y_files[0], mmap_mode=\"r\")\n",
        "pos_rate = max(1e-6, float(np.mean(first_y)))\n",
        "pos_w = (1.0 - pos_rate) / pos_rate\n",
        "print(f\"Estimated pos_rate: {pos_rate:.6f}  pos_weight={pos_w:.2f}\")\n",
        "\n",
        "n_features = X_val.shape[1]\n",
        "model = FTTransformer(n_features).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=device))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "\n",
        "# autocast helper\n",
        "if device == \"cuda\":\n",
        "    from torch.amp import autocast, GradScaler\n",
        "    scaler = GradScaler(\"cuda\")\n",
        "else:\n",
        "    class _NoOp:\n",
        "        def __enter__(self): return None\n",
        "        def __exit__(self, *a): return False\n",
        "    def autocast(*a, **k): return _NoOp()\n",
        "    scaler = None\n",
        "\n",
        "def eval_val(model):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        CH = 2048   # smaller eval chunk = lower RAM\n",
        "        for s in range(0, X_val.shape[0], CH):\n",
        "            xb = torch.from_numpy(np.asarray(X_val[s:s+CH], dtype=np.float32)).to(device)\n",
        "            with autocast(\"cuda\"):\n",
        "                p = torch.sigmoid(model(xb)).float().cpu().numpy()\n",
        "            preds.append(p)\n",
        "    return float(roc_auc_score(y_val, np.concatenate(preds))), np.concatenate(preds)\n",
        "\n",
        "best_auc, bad = -1.0, 0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train(); t0=time.time()\n",
        "    running, step, accum = 0.0, 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device=device, dtype=torch.float32, non_blocking=False)\n",
        "        yb = yb.to(device=device, dtype=torch.float32, non_blocking=False)\n",
        "        with autocast(\"cuda\"):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb) / ACCUM\n",
        "        if scaler: scaler.scale(loss).backward()\n",
        "        else:      loss.backward()\n",
        "        accum += 1\n",
        "        if accum % ACCUM == 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            if scaler:\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "        running += float(loss.item())*xb.size(0); step += 1\n",
        "        if step % 100 == 0:\n",
        "            print(f\"  step {step}  (loss chunk avg ~ {running/max(1,step*BATCH):.4f})\")\n",
        "        if step >= MAX_BATCHES_PER_EPOCH:\n",
        "            print(f\"Reached MAX_BATCHES_PER_EPOCH={MAX_BATCHES_PER_EPOCH}; ending epoch early.\")\n",
        "            break\n",
        "\n",
        "    if accum % ACCUM != 0:\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        if scaler: scaler.step(optimizer); scaler.update()\n",
        "        else:      optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    auc, p_val = eval_val(model)\n",
        "    print(f\"[{epoch}] train_loss≈{running/max(1,(step*BATCH)):.4f}  val_auc={auc:.6f}\")\n",
        "\n",
        "    if auc > best_auc + 1e-4:\n",
        "        best_auc, bad = auc, 0\n",
        "        best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "        np.save(ftdir/\"ft_val_proba.npy\", p_val)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "torch.save(model.state_dict(), ftdir/\"ft_model.pt\")\n",
        "print(f\"TRAIN DONE → best val AUC: {best_auc:.6f}. Saved: /content/ft90/ft_model.pt and ft_val_proba.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26-OJsIkuUBS"
      },
      "source": [
        "# STEP 7B (90d): evaluate FT on validation, per-trip preview, per-user FT scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J67uqNfYrA7k"
      },
      "outputs": [],
      "source": [
        "# STEP 7B (90d): evaluate FT on validation, per-trip preview, per-user FT scores\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "FTDIR = Path(\"/content/ft90\")\n",
        "assert (FTDIR/\"ft_val_proba.npy\").exists() and (FTDIR/\"y_val.npy\").exists() and (FTDIR/\"val_ids.parquet\").exists(), \"Run 7A-TRAIN first.\"\n",
        "\n",
        "p_val  = np.load(FTDIR/\"ft_val_proba.npy\")\n",
        "y_val  = np.load(FTDIR/\"y_val.npy\")\n",
        "val_ids = pd.read_parquet(FTDIR/\"val_ids.parquet\", engine=\"pyarrow\")  # trip_id,user_id,incident_flag\n",
        "assert len(p_val) == len(y_val) == len(val_ids), \"Length mismatch; re-run 7A-PREP.\"\n",
        "\n",
        "print({\"AUROC\": float(roc_auc_score(y_val, p_val)),\n",
        "       \"PR_AUC\": float(average_precision_score(y_val, p_val))})\n",
        "\n",
        "tmp = val_ids.copy()\n",
        "tmp[\"proba\"] = p_val.astype(\"float32\")\n",
        "\n",
        "def nova_score_from_proba(p, beta0=-3.5, beta1=10.0):\n",
        "    s = 1.0 / (1.0 + np.exp(-(beta0 - beta1 * p)))\n",
        "    return 300.0 + 600.0 * s\n",
        "\n",
        "def decision_band(s):\n",
        "    if s < 600:  return \"<600 (Decline + coaching)\"\n",
        "    if s < 680:  return \"600–679 (Caution)\"\n",
        "    if s < 740:  return \"680–739 (Manual review)\"\n",
        "    return \"≥740 (Approve)\"\n",
        "\n",
        "tmp[\"nova_score\"] = nova_score_from_proba(tmp[\"proba\"]).astype(\"float32\")\n",
        "tmp[\"decision_band\"] = [decision_band(s) for s in tmp[\"nova_score\"]]\n",
        "\n",
        "# tiny per-trip preview\n",
        "tmp.sample(min(10, len(tmp)), random_state=7).to_csv(\"/content/val_preds_90d_ft_sample.csv\", index=False)\n",
        "print(\"Saved → /content/val_preds_90d_ft_sample.csv\")\n",
        "\n",
        "# per-user FT scores\n",
        "ft_users = (tmp.groupby(\"user_id\", as_index=False)\n",
        "            .agg(proba_mean=(\"proba\",\"mean\"), trips=(\"proba\",\"size\")))\n",
        "ft_users[\"nova_score\"] = nova_score_from_proba(ft_users[\"proba_mean\"]).astype(\"float32\")\n",
        "ft_users[\"decision_band\"] = [decision_band(s) for s in ft_users[\"nova_score\"]]\n",
        "\n",
        "out_ft = \"/content/ft90/user_scores_90d_ft.csv\"\n",
        "ft_users.to_csv(out_ft, index=False)\n",
        "print(f\"Saved → {out_ft}\")\n",
        "display(ft_users.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nVJsnpn7VT"
      },
      "source": [
        "# STEP 7E (ROBUST) — user-level aggregation + baseline stacking with single-class safety (90d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i-D26jFln4TD"
      },
      "outputs": [],
      "source": [
        "# STEP 7C — user-level ensemble (FT + Node2Vec) for 90d\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "FT_USERS  = Path(\"/content/ft90/user_scores_90d_ft.csv\")      # from 7B\n",
        "N2V_USERS = Path(\"/content/user_scores_90d_n2v_cal.csv\")      # from 6D (or /content/user_scores_90d_n2v.csv if you skipped 6D)\n",
        "OUT_PATH  = Path(\"/content/ensemble_user_scores_90d.csv\")\n",
        "\n",
        "assert FT_USERS.exists(), \"Missing FT user scores. Run 7B first.\"\n",
        "ft = pd.read_csv(FT_USERS, dtype={\"user_id\":\"string\"})\n",
        "\n",
        "# Prefer calibrated N2V if available; else fall back to raw N2V\n",
        "use_n2v = N2V_USERS.exists()\n",
        "if use_n2v:\n",
        "    n2v = pd.read_csv(N2V_USERS, dtype={\"user_id\":\"string\"})\n",
        "    # keep only what we need and rename proba to avoid collisions\n",
        "    base_cols = {\"user_id\",\"proba_mean\",\"trips\"}\n",
        "    missing = base_cols - set(n2v.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"N2V file missing columns: {missing}\")\n",
        "    n2v = n2v[[\"user_id\",\"proba_mean\",\"trips\"]].rename(\n",
        "        columns={\"proba_mean\":\"proba_mean_n2v\",\"trips\":\"trips_n2v\"}\n",
        "    )\n",
        "    df = ft.merge(n2v, on=\"user_id\", how=\"left\")\n",
        "else:\n",
        "    df = ft.copy()\n",
        "    df[\"proba_mean_n2v\"] = np.nan\n",
        "    df[\"trips_n2v\"] = np.nan\n",
        "\n",
        "# blend weights (adjust if you prefer)\n",
        "W_FT, W_N2V = 0.6, 0.4\n",
        "\n",
        "# final probability per user\n",
        "df[\"proba_blend\"] = np.where(\n",
        "    df[\"proba_mean_n2v\"].notna(),\n",
        "    W_FT*df[\"proba_mean\"].values + W_N2V*df[\"proba_mean_n2v\"].values,\n",
        "    df[\"proba_mean\"].values,  # fallback to FT when N2V missing\n",
        ")\n",
        "\n",
        "# Nova score mapping (higher risk -> lower score), temperature tunes spread (keep modest here)\n",
        "def nova_score_from_proba(p, temperature=0.35):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    z = (0.5 - p) / temperature\n",
        "    return 300.0 + 600.0 * (1.0 / (1.0 + np.exp(-z)))\n",
        "\n",
        "def band_from_score(s):\n",
        "    if s >= 750: return \"≥750 (Auto-approve)\"\n",
        "    if s >= 700: return \"700–749 (Standard approve)\"\n",
        "    if s >= 600: return \"600–699 (Manual review)\"\n",
        "    return \"<600 (Decline + coaching)\"\n",
        "\n",
        "df[\"nova_score\"]    = nova_score_from_proba(df[\"proba_blend\"].values)\n",
        "df[\"decision_band\"] = df[\"nova_score\"].apply(band_from_score)\n",
        "\n",
        "keep_cols = [\"user_id\", \"proba_mean\", \"proba_mean_n2v\", \"proba_blend\",\n",
        "             \"trips\", \"trips_n2v\", \"nova_score\",\"decision_band\"]\n",
        "out = df[keep_cols].copy()\n",
        "\n",
        "out.to_csv(OUT_PATH, index=False)\n",
        "print(f\"Saved → {OUT_PATH}\")\n",
        "print(out.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nBand distribution (ensemble):\")\n",
        "print(out[\"decision_band\"].value_counts().to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPd0PcWxoLHY"
      },
      "source": [
        "# STEP 7C — user-level ensemble (FT + Node2Vec) for 90d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Iln316YquX9B"
      },
      "outputs": [],
      "source": [
        "# STEP 7C — user-level ensemble (FT + Node2Vec) for 90d\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "FT_USERS  = Path(\"/content/ft90/user_scores_90d_ft.csv\")     # from 7B\n",
        "N2V_USERS = Path(\"/content/user_scores_90d_n2v_cal.csv\")     # from 6D\n",
        "OUT_PATH  = Path(\"/content/ensemble_user_scores_90d.csv\")\n",
        "\n",
        "assert FT_USERS.exists(), \"Missing FT user scores. Run 7B first.\"\n",
        "ft = pd.read_csv(FT_USERS, dtype={\"user_id\":\"string\"})\n",
        "\n",
        "# optional Node2Vec (calibrated) — if missing, we just pass through FT\n",
        "use_n2v = N2V_USERS.exists()\n",
        "if use_n2v:\n",
        "    n2v = pd.read_csv(N2V_USERS, dtype={\"user_id\":\"string\"})\n",
        "    n2v = n2v[[\"user_id\",\"proba_mean\",\"trips\"]].rename(\n",
        "        columns={\"proba_mean\":\"proba_mean_n2v\",\"trips\":\"trips_n2v\"}\n",
        "    )\n",
        "    df = ft.merge(n2v, on=\"user_id\", how=\"left\")\n",
        "else:\n",
        "    df = ft.copy()\n",
        "    df[\"proba_mean_n2v\"] = np.nan\n",
        "\n",
        "# blend weights (adjust if you prefer)\n",
        "W_FT, W_N2V = 0.6, 0.4\n",
        "\n",
        "# final probability per user\n",
        "df[\"proba_blend\"] = np.where(\n",
        "    df[\"proba_mean_n2v\"].notna(),\n",
        "    W_FT*df[\"proba_mean\"].values + W_N2V*df[\"proba_mean_n2v\"].values,\n",
        "    df[\"proba_mean\"].values,  # fallback to FT when no N2V\n",
        ")\n",
        "\n",
        "# Nova score mapping (higher risk -> lower score) 300–900\n",
        "def nova_score_from_proba(p, temperature=0.35):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    z = (0.5 - p) / temperature\n",
        "    return 300.0 + 600.0 * (1.0 / (1.0 + np.exp(-z)))\n",
        "\n",
        "def band_from_score(s):\n",
        "    if s >= 750: return \"≥750 (Auto-approve)\"\n",
        "    if s >= 700: return \"700–749 (Standard approve)\"\n",
        "    if s >= 600: return \"600–699 (Manual review)\"\n",
        "    return \"<600 (Decline + coaching)\"\n",
        "\n",
        "df[\"nova_score\"]    = nova_score_from_proba(df[\"proba_blend\"].values)\n",
        "df[\"decision_band\"] = df[\"nova_score\"].apply(band_from_score)\n",
        "\n",
        "# tidy output table\n",
        "keep_cols = [\"user_id\", \"proba_mean\", \"proba_mean_n2v\", \"proba_blend\",\n",
        "             \"trips\"] + ([\"trips_n2v\"] if use_n2v else []) + [\"nova_score\",\"decision_band\"]\n",
        "out = df[keep_cols].copy()\n",
        "\n",
        "out.to_csv(OUT_PATH, index=False)\n",
        "print(f\"Saved → {OUT_PATH}\")\n",
        "print(out.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nBand distribution (ensemble):\")\n",
        "print(out[\"decision_band\"].value_counts().to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyGQ1QxTqhWy"
      },
      "source": [
        "# STEP 7F-PREP-TCN — build weekly user sequences (90d) for a temporal model (TCN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tsu7aYBJqe5a"
      },
      "outputs": [],
      "source": [
        "# STEP 7F-PREP-TCN — build weekly user sequences (90d) for a temporal model (TCN)\n",
        "# Inputs: /content/model_table_90d.parquet\n",
        "# Outputs under /content/tcn90/: X_train_seq.npy, y_train.npy, X_val_seq.npy, y_val.npy,\n",
        "#                               users_train.csv, users_val.csv, feat_names.json\n",
        "\n",
        "import numpy as np, pandas as pd, json, gc\n",
        "from pathlib import Path\n",
        "\n",
        "PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "assert PARQ.exists(), \"Missing model_table_90d.parquet. Run STEP 4 (90d) first.\"\n",
        "\n",
        "outdir = Path(\"/content/tcn90\"); outdir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# 1) Load minimal columns\n",
        "use_cols = [\n",
        "    \"user_id\",\"trip_date_dt\",\"incident_flag\",\n",
        "    \"trip_duration\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"trip_rating\",\n",
        "    \"cancellation_flag\",\"safety_score\",\"speed_kmph\",\"fare_per_km\",\"tip_pct\"\n",
        "]\n",
        "df = pd.read_parquet(PARQ, engine=\"pyarrow\", columns=[c for c in use_cols if c in pd.read_parquet(PARQ, engine=\"pyarrow\").columns])\n",
        "\n",
        "# types\n",
        "df[\"user_id\"] = df[\"user_id\"].astype(\"string\")\n",
        "df[\"trip_date_dt\"] = pd.to_datetime(df[\"trip_date_dt\"], errors=\"coerce\")\n",
        "for c in df.columns:\n",
        "    if c not in (\"user_id\",\"trip_date_dt\",\"incident_flag\") and pd.api.types.is_numeric_dtype(df[c]) is False:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "df[\"incident_flag\"] = df[\"incident_flag\"].astype(\"int8\")\n",
        "\n",
        "# 2) Chronological split (same 70/30 logic as FT)\n",
        "if str(df[\"trip_date_dt\"].dtype).startswith(\"datetime64[ns,\"):\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"].dt.tz_localize(None)\n",
        "else:\n",
        "    df[\"trip_date_naive\"] = df[\"trip_date_dt\"]\n",
        "\n",
        "cutoff = df[\"trip_date_naive\"].quantile(0.70)\n",
        "print(\"7F cutoff:\", cutoff)\n",
        "last_trip = df.groupby(\"user_id\", observed=True)[\"trip_date_naive\"].max().rename(\"last_trip\").reset_index()\n",
        "train_users = set(last_trip.loc[last_trip[\"last_trip\"] <= cutoff, \"user_id\"].astype(\"string\"))\n",
        "val_users   = set(last_trip.loc[last_trip[\"last_trip\"] >  cutoff, \"user_id\"].astype(\"string\"))\n",
        "\n",
        "# 3) Weekly bucket (Mon-start weeks), choose a fixed horizon H weeks (e.g., 12)\n",
        "H = 12\n",
        "week = df[\"trip_date_naive\"].dt.to_period(\"W-MON\").apply(lambda p: p.start_time)\n",
        "df = df.assign(week_start=week)\n",
        "\n",
        "# 4) Aggregate per user-week (keep compact, stable numerics)\n",
        "agg = df.groupby([\"user_id\",\"week_start\"], observed=True).agg(\n",
        "    trips=(\"incident_flag\",\"size\"),\n",
        "    incidents=(\"incident_flag\",\"sum\"),\n",
        "    fare_sum=(\"fare_amount\",\"sum\"),\n",
        "    fare_mean=(\"fare_amount\",\"mean\"),\n",
        "    dist_mean=(\"trip_distance\",\"mean\"),\n",
        "    dur_mean=(\"trip_duration\",\"mean\"),\n",
        "    rating_mean=(\"trip_rating\",\"mean\"),\n",
        "    safety_mean=(\"safety_score\",\"mean\"),\n",
        "    tip_mean=(\"tip_amount\",\"mean\"),\n",
        "    tip_pct_mean=(\"tip_pct\",\"mean\"),\n",
        "    cancel_rate=(\"cancellation_flag\",\"mean\"),\n",
        "    speed_mean=(\"speed_kmph\",\"mean\"),\n",
        "    fpk_mean=(\"fare_per_km\",\"mean\"),\n",
        ").reset_index()\n",
        "\n",
        "# 5) Normalize weeks to a shared window (last H weeks seen globally)\n",
        "global_last_week = agg[\"week_start\"].max()\n",
        "week_ends = pd.date_range(end=global_last_week, periods=H, freq=\"W-MON\")\n",
        "week_starts = (week_ends - pd.to_timedelta(7, unit=\"D\"))  # start of each Mon-week\n",
        "week_starts = pd.to_datetime(week_starts)\n",
        "\n",
        "# 6) Build dense sequences: for each user, align to these H week_starts\n",
        "feat_cols = [\"trips\",\"incidents\",\"fare_sum\",\"fare_mean\",\"dist_mean\",\"dur_mean\",\n",
        "             \"rating_mean\",\"safety_mean\",\"tip_mean\",\"tip_pct_mean\",\"cancel_rate\",\n",
        "             \"speed_mean\",\"fpk_mean\"]\n",
        "D = len(feat_cols)\n",
        "\n",
        "def build_seq(group_df):\n",
        "    # map from week_start -> feature row\n",
        "    wk_map = group_df.set_index(\"week_start\")[feat_cols]\n",
        "    mat = np.zeros((H, D), dtype=np.float32)\n",
        "    for i, ws in enumerate(week_starts):\n",
        "        row = wk_map.loc[ws] if ws in wk_map.index else None\n",
        "        if row is not None:\n",
        "            mat[i, :] = row.fillna(0.0).to_numpy(dtype=np.float32)\n",
        "        # else zeros\n",
        "    return mat\n",
        "\n",
        "# Split users first to avoid building for everyone twice\n",
        "agg[\"user_id\"] = agg[\"user_id\"].astype(\"string\")\n",
        "agg_train = agg[agg[\"user_id\"].isin(train_users)]\n",
        "agg_val   = agg[agg[\"user_id\"].isin(val_users)]\n",
        "\n",
        "# labels: ANY incident within the H-week window we just aligned\n",
        "def labels_from_window(group_df):\n",
        "    # any incident in those H weeks\n",
        "    return int(group_df[\"incidents\"].fillna(0).sum() > 0)\n",
        "\n",
        "# Build sequences per user (train)\n",
        "train_list, train_labels, train_uids = [], [], []\n",
        "for uid, g in agg_train.groupby(\"user_id\", observed=True):\n",
        "    seq = build_seq(g)\n",
        "    train_list.append(seq)\n",
        "    train_labels.append(labels_from_window(g))\n",
        "    train_uids.append(str(uid))\n",
        "\n",
        "# Build sequences per user (val)\n",
        "val_list, val_labels, val_uids = [], [], []\n",
        "for uid, g in agg_val.groupby(\"user_id\", observed=True):\n",
        "    seq = build_seq(g)\n",
        "    val_list.append(seq)\n",
        "    val_labels.append(labels_from_window(g))\n",
        "    val_uids.append(str(uid))\n",
        "\n",
        "X_train = np.stack(train_list, axis=0) if train_list else np.zeros((0,H,D), np.float32)\n",
        "X_val   = np.stack(val_list,   axis=0) if val_list   else np.zeros((0,H,D), np.float32)\n",
        "y_train = np.asarray(train_labels, dtype=np.int8)\n",
        "y_val   = np.asarray(val_labels,   dtype=np.int8)\n",
        "\n",
        "# 7) Save arrays + user lists + feature names\n",
        "np.save(outdir/\"X_train_seq.npy\", X_train)\n",
        "np.save(outdir/\"y_train.npy\",     y_train)\n",
        "np.save(outdir/\"X_val_seq.npy\",   X_val)\n",
        "np.save(outdir/\"y_val.npy\",       y_val)\n",
        "\n",
        "pd.DataFrame({\"user_id\": train_uids}).to_csv(outdir/\"users_train.csv\", index=False)\n",
        "pd.DataFrame({\"user_id\": val_uids}).to_csv(outdir/\"users_val.csv\", index=False)\n",
        "with open(outdir/\"feat_names.json\",\"w\") as f:\n",
        "    json.dump({\"feat_cols\": feat_cols, \"H\": H}, f)\n",
        "\n",
        "gc.collect()\n",
        "print(\"7F-PREP-TCN DONE\")\n",
        "print(\"Train seq:\", X_train.shape, \"pos:\", int(y_train.sum()), \"neg:\", int((y_train==0).sum()))\n",
        "print(\"Val   seq:\", X_val.shape,   \"pos:\", int(y_val.sum()),   \"neg:\", int((y_val==0).sum()))\n",
        "print(\"Saved to:\", outdir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpaj6AAA8vvo"
      },
      "source": [
        "# STEP 7F-PREP-TCN-FIX-v2 — auto-pick windows & label threshold (ensures train users & both classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WbbL4sln8t73"
      },
      "outputs": [],
      "source": [
        "# STEP 7F-PREP-TCN-FIX-v3 — derive windows from actual weeks so we get users on both sides\n",
        "# Inputs:  /content/model_table_90d.parquet\n",
        "# Outputs: /content/tcn90/{X_train_seq.npy,y_train.npy,X_val_seq.npy,y_val.npy,\n",
        "#                       users_train.csv,users_val.csv,feat_names.json,label_info.json}\n",
        "\n",
        "import numpy as np, pandas as pd, json, gc\n",
        "from pathlib import Path\n",
        "\n",
        "PARQ = Path(\"/content/model_table_90d.parquet\")\n",
        "assert PARQ.exists(), \"Missing model_table_90d.parquet. Run STEP 4 (90d) first.\"\n",
        "outdir = Path(\"/content/tcn90\"); outdir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# 1) Load minimal columns\n",
        "need_cols = [\n",
        "    \"user_id\",\"trip_date_dt\",\"incident_flag\",\n",
        "    \"trip_duration\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"trip_rating\",\n",
        "    \"cancellation_flag\",\"safety_score\",\"speed_kmph\",\"fare_per_km\",\"tip_pct\"\n",
        "]\n",
        "cols_avail = pd.read_parquet(PARQ, engine=\"pyarrow\").columns\n",
        "use_cols = [c for c in need_cols if c in cols_avail]\n",
        "df = pd.read_parquet(PARQ, engine=\"pyarrow\", columns=use_cols)\n",
        "\n",
        "df[\"user_id\"] = df[\"user_id\"].astype(\"string\")\n",
        "df[\"trip_date_dt\"] = pd.to_datetime(df[\"trip_date_dt\"], errors=\"coerce\")\n",
        "df[\"incident_flag\"] = pd.to_numeric(df[\"incident_flag\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
        "for c in df.columns:\n",
        "    if c not in (\"user_id\",\"trip_date_dt\",\"incident_flag\"):\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# 2) Weeks + per-user-week aggregation (use actual weeks present)\n",
        "dt_naive = df[\"trip_date_dt\"].dt.tz_localize(None) if str(df[\"trip_date_dt\"].dtype).startswith(\"datetime64[ns,\") else df[\"trip_date_dt\"]\n",
        "df[\"week_start\"] = dt_naive.dt.to_period(\"W-MON\").apply(lambda p: p.start_time)\n",
        "\n",
        "agg = df.groupby([\"user_id\",\"week_start\"], observed=True).agg(\n",
        "    trips=(\"incident_flag\",\"size\"),\n",
        "    incidents=(\"incident_flag\",\"sum\"),\n",
        "    fare_sum=(\"fare_amount\",\"sum\"),\n",
        "    fare_mean=(\"fare_amount\",\"mean\"),\n",
        "    dist_mean=(\"trip_distance\",\"mean\"),\n",
        "    dur_mean=(\"trip_duration\",\"mean\"),\n",
        "    rating_mean=(\"trip_rating\",\"mean\"),\n",
        "    safety_mean=(\"safety_score\",\"mean\"),\n",
        "    tip_mean=(\"tip_amount\",\"mean\"),\n",
        "    tip_pct_mean=(\"tip_pct\",\"mean\"),\n",
        "    cancel_rate=(\"cancellation_flag\",\"mean\"),\n",
        "    speed_mean=(\"speed_kmph\",\"mean\"),\n",
        "    fpk_mean=(\"fare_per_km\",\"mean\"),\n",
        ").reset_index()\n",
        "agg[\"user_id\"] = agg[\"user_id\"].astype(\"string\")\n",
        "\n",
        "feat_cols = [\"trips\",\"incidents\",\"fare_sum\",\"fare_mean\",\"dist_mean\",\"dur_mean\",\n",
        "             \"rating_mean\",\"safety_mean\",\"tip_mean\",\"tip_pct_mean\",\"cancel_rate\",\n",
        "             \"speed_mean\",\"fpk_mean\"]\n",
        "D = len(feat_cols)\n",
        "\n",
        "# 3) Build windows from the actual unique weeks in data\n",
        "weeks_all = np.array(sorted(agg[\"week_start\"].unique()))\n",
        "assert weeks_all.size > 0, \"No weekly data found.\"\n",
        "\n",
        "# choose up to 8 weeks for val (last weeks), and up to 8 weeks just before that for train\n",
        "H_val = min(8, weeks_all.size // 2) or 1\n",
        "val_weeks = weeks_all[-H_val:]\n",
        "before_val = weeks_all[: -H_val] if H_val < weeks_all.size else np.array([], dtype=\"datetime64[ns]\")\n",
        "H_train = min(8, before_val.size) or min(1, weeks_all.size)  # at least 1 if anything exists\n",
        "train_weeks = before_val[-H_train:] if before_val.size else weeks_all[:H_train]\n",
        "\n",
        "print(f\"[7F-FIX-v3] weeks_all={weeks_all.size} | H_train={H_train} | H_val={H_val}\")\n",
        "print(f\"[7F-FIX-v3] train range: {train_weeks[0] if train_weeks.size else 'n/a'} → {train_weeks[-1] if train_weeks.size else 'n/a'}\")\n",
        "print(f\"[7F-FIX-v3]   val range: {val_weeks[0] if val_weeks.size else 'n/a'} → {val_weeks[-1] if val_weeks.size else 'n/a'}\")\n",
        "\n",
        "agg_t = agg[agg[\"week_start\"].isin(train_weeks)].copy()\n",
        "agg_v = agg[agg[\"week_start\"].isin(val_weeks)].copy()\n",
        "print(f\"[7F-FIX-v3] train users={agg_t['user_id'].nunique()} | val users={agg_v['user_id'].nunique()}\")\n",
        "\n",
        "# 4) Build dense sequences aligned to their own windows\n",
        "def build_seq_for_user(gdf, weeks_sorted):\n",
        "    m = np.zeros((len(weeks_sorted), D), dtype=np.float32)\n",
        "    mdf = gdf.set_index(\"week_start\")[feat_cols]\n",
        "    for i, ws in enumerate(weeks_sorted):\n",
        "        if ws in mdf.index:\n",
        "            m[i,:] = mdf.loc[ws].fillna(0.0).to_numpy(dtype=np.float32)\n",
        "    return m\n",
        "\n",
        "def make_side(agg_side, weeks_sorted):\n",
        "    seqs, counts, uids = [], [], []\n",
        "    for uid, g in agg_side.groupby(\"user_id\", observed=True):\n",
        "        seqs.append(build_seq_for_user(g, weeks_sorted))\n",
        "        counts.append(float(g[\"incidents\"].fillna(0).sum()))\n",
        "        uids.append(str(uid))\n",
        "    X = np.stack(seqs, axis=0).astype(np.float32) if seqs else np.zeros((0, len(weeks_sorted), D), np.float32)\n",
        "    return X, np.asarray(counts, dtype=np.float32), uids\n",
        "\n",
        "X_train, cnt_train, u_train = make_side(agg_t, list(train_weeks))\n",
        "X_val,   cnt_val,   u_val   = make_side(agg_v,  list(val_weeks))\n",
        "\n",
        "print(f\"[7F-FIX-v3] X_train: {X_train.shape} | X_val: {X_val.shape}\")\n",
        "\n",
        "# 5) Pick incidents threshold k to create both classes (if possible)\n",
        "def pick_k(counts):\n",
        "    if counts.size == 0: return 1\n",
        "    for k in [1,2,3,4,5]:\n",
        "        rate = float((counts >= k).mean())\n",
        "        if 0.05 <= rate <= 0.95:\n",
        "            return k\n",
        "    # fallback to 80th percentile (>=2 typically)\n",
        "    return int(max(1, np.ceil(np.quantile(counts, 0.80))))\n",
        "\n",
        "combined = np.concatenate([cnt_train, cnt_val]) if cnt_train.size + cnt_val.size else np.array([0.0])\n",
        "k = pick_k(combined)\n",
        "y_train = (cnt_train >= k).astype(np.int8) if cnt_train.size else np.zeros((0,), np.int8)\n",
        "y_val   = (cnt_val   >= k).astype(np.int8) if cnt_val.size else np.zeros((0,), np.int8)\n",
        "\n",
        "print(f\"[7F-FIX-v3] label k={k} | train pos={int(y_train.sum())}/{len(y_train)} | val pos={int(y_val.sum())}/{len(y_val)}\")\n",
        "\n",
        "# 6) Save\n",
        "np.save(outdir/\"X_train_seq.npy\", X_train)\n",
        "np.save(outdir/\"y_train.npy\",     y_train)\n",
        "np.save(outdir/\"X_val_seq.npy\",   X_val)\n",
        "np.save(outdir/\"y_val.npy\",       y_val)\n",
        "\n",
        "pd.DataFrame({\"user_id\": u_train}).to_csv(outdir/\"users_train.csv\", index=False)\n",
        "pd.DataFrame({\"user_id\": u_val}).to_csv(outdir/\"users_val.csv\", index=False)\n",
        "with open(outdir/\"feat_names.json\",\"w\") as f:\n",
        "    json.dump({\"feat_cols\": feat_cols, \"H_train\": int(H_train), \"H_val\": int(H_val)}, f)\n",
        "with open(outdir/\"label_info.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"k_incidents\": int(k),\n",
        "        \"train_weeks\": [str(x) for x in train_weeks],\n",
        "        \"val_weeks\":   [str(x) for x in val_weeks]\n",
        "    }, f)\n",
        "\n",
        "gc.collect()\n",
        "print(\"7F-PREP-TCN-FIX-v3 DONE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjr-hRrHED6N"
      },
      "source": [
        "# STEP 7F-TCN-TRAIN — train a light TCN on the sequences from 7F-PREP-TCN-FIX-v3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "efosmvP-EDbq"
      },
      "outputs": [],
      "source": [
        "# STEP 7F-TCN-TRAIN — train a light TCN on the sequences from 7F-PREP-TCN-FIX-v3\n",
        "\n",
        "import numpy as np, pandas as pd, torch, torch.nn as nn, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "tcn_dir = Path(\"/content/tcn90\")\n",
        "X_train = np.load(tcn_dir/\"X_train_seq.npy\", mmap_mode=\"r\")\n",
        "y_train = np.load(tcn_dir/\"y_train.npy\")\n",
        "X_val   = np.load(tcn_dir/\"X_val_seq.npy\",   mmap_mode=\"r\")\n",
        "y_val   = np.load(tcn_dir/\"y_val.npy\")\n",
        "print(\"Train:\", X_train.shape, \"| Val:\", X_val.shape)\n",
        "\n",
        "H = X_val.shape[1] if X_val.ndim==3 else 0\n",
        "D = X_val.shape[2] if X_val.ndim==3 else 0\n",
        "assert H>0 and D>0, \"Empty sequences. Re-run 7F-PREP-TCN-FIX-v3 if this triggers.\"\n",
        "\n",
        "class SmallTCN(nn.Module):\n",
        "    def __init__(self, D, hidden=64, k=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(D, hidden, kernel_size=k, padding=\"same\", dilation=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(hidden, hidden, kernel_size=k, padding=\"same\", dilation=2),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(hidden, hidden, kernel_size=k, padding=\"same\", dilation=4),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "    def forward(self, x):          # x: (N, H, D)\n",
        "        x = x.transpose(1,2)       # -> (N, D, H)\n",
        "        z = self.net(x)            # (N, hidden, H)\n",
        "        z = torch.amax(z, dim=-1)  # global max pool over time\n",
        "        return self.head(z).squeeze(-1)\n",
        "\n",
        "model = SmallTCN(D).to(device)\n",
        "\n",
        "# class-weighted BCE\n",
        "pos_rate = float(max(1e-6, y_train.mean())) if len(y_train) else 0.5\n",
        "pos_w = (1.0 - pos_rate) / max(1e-6, pos_rate)\n",
        "crit  = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=device))\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "BATCH, EPOCHS, PATIENCE = 256, 10, 2\n",
        "\n",
        "def batches(X, y, bs=BATCH, shuffle=True):\n",
        "    n = X.shape[0]; order = np.arange(n)\n",
        "    if shuffle: np.random.default_rng(42).shuffle(order)\n",
        "    for s in range(0, n, bs):\n",
        "        idx = order[s:s+bs]\n",
        "        xb = torch.from_numpy(np.asarray(X[idx], dtype=np.float32)).to(device)\n",
        "        yb = torch.from_numpy(np.asarray(y[idx], dtype=np.float32)).to(device)\n",
        "        yield xb, yb\n",
        "\n",
        "def eval_val():\n",
        "    model.eval(); preds=[]; CH=4096\n",
        "    with torch.no_grad():\n",
        "        for s in range(0, X_val.shape[0], CH):\n",
        "            xb = torch.from_numpy(np.asarray(X_val[s:s+CH], dtype=np.float32)).to(device)\n",
        "            p = torch.sigmoid(model(xb)).float().cpu().numpy()\n",
        "            preds.append(p)\n",
        "    p = np.concatenate(preds)\n",
        "    ok = (np.unique(y_val).size == 2)\n",
        "    auc = float(roc_auc_score(y_val, p)) if ok else float(\"nan\")\n",
        "    pr  = float(average_precision_score(y_val, p)) if ok else float(\"nan\")\n",
        "    return auc, pr, p\n",
        "\n",
        "best, bad, best_state = -1.0, 0, None\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train(); t0=time.time(); run=0.0; seen=0\n",
        "    for xb, yb in batches(X_train, y_train):\n",
        "        logits = model(xb); loss = crit(logits, yb)\n",
        "        loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step(); opt.zero_grad()\n",
        "        run += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
        "    auc, pr, p_val = eval_val()\n",
        "    print(f\"[{ep}] train_loss≈{(run/max(1,seen)):.4f}  val_auc={auc:.6f}  pr_auc={pr:.6f}  time={(time.time()-t0):.1f}s\")\n",
        "    score = (auc if not np.isnan(auc) else -(run/max(1,seen)))\n",
        "    if score > best + 1e-4:\n",
        "        best, bad, best_state = score, 0, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "        np.save(tcn_dir/\"tcn_val_proba.npy\", p_val)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "if best_state: model.load_state_dict(best_state)\n",
        "torch.save(model.state_dict(), tcn_dir/\"tcn_model.pt\")\n",
        "\n",
        "# Save per-user validation predictions\n",
        "u_val = pd.read_csv(tcn_dir/\"users_val.csv\")[\"user_id\"].astype(str).values\n",
        "p_val = np.load(tcn_dir/\"tcn_val_proba.npy\")\n",
        "out = pd.DataFrame({\"user_id\": u_val, \"proba_tcn\": p_val, \"y_true\": y_val})\n",
        "out.to_csv(tcn_dir/\"user_scores_90d_tcn.csv\", index=False)\n",
        "print(\"Saved →\", tcn_dir/\"user_scores_90d_tcn.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHnYqknKExbZ"
      },
      "source": [
        "# STEP 7G — stack TCN + FT + N2V (user-level) and report metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "80wQ0c2mEuiw"
      },
      "outputs": [],
      "source": [
        "# STEP 7G — stack TCN + FT + N2V (user-level) and report metrics\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def approx_any(mean_p, n):\n",
        "    mean_p = np.clip(pd.Series(mean_p, dtype=\"float64\").values, 0, 1)\n",
        "    n = np.maximum(pd.Series(n).fillna(1).astype(int).values, 1)\n",
        "    return 1.0 - np.power(1.0 - mean_p, n)\n",
        "\n",
        "# ---- Inputs ----\n",
        "tcn = pd.read_csv(\"/content/tcn90/user_scores_90d_tcn.csv\")     # user_id, proba_tcn, y_true\n",
        "ft  = pd.read_csv(\"/content/ft90/user_scores_90d_ft.csv\")       # user_id, proba_mean, trips\n",
        "n2v_path_cal = Path(\"/content/user_scores_90d_n2v_cal.csv\")\n",
        "n2v_path_raw = Path(\"/content/user_scores_90d_n2v.csv\")\n",
        "if n2v_path_cal.exists():\n",
        "    n2v = pd.read_csv(n2v_path_cal)\n",
        "elif n2v_path_raw.exists():\n",
        "    n2v = pd.read_csv(n2v_path_raw)\n",
        "else:\n",
        "    n2v = None\n",
        "\n",
        "# ---- Build per-user \"any incident\" probabilities for FT / N2V ----\n",
        "ft[\"user_id\"] = ft[\"user_id\"].astype(str)\n",
        "ft[\"p_ft\"] = approx_any(ft[\"proba_mean\"], ft[\"trips\"])\n",
        "\n",
        "if n2v is not None:\n",
        "    n2v[\"user_id\"] = n2v[\"user_id\"].astype(str)\n",
        "    src_p = \"proba_mean\" if \"proba_mean\" in n2v.columns else (\"proba\" if \"proba\" in n2v.columns else None)\n",
        "    src_n = \"trips\" if \"trips\" in n2v.columns else None\n",
        "    if src_p is None:\n",
        "        n2v = pd.DataFrame({\"user_id\": [], \"p_n2v\": []})\n",
        "    else:\n",
        "        if src_n is None:\n",
        "            n2v[\"p_n2v\"] = np.clip(n2v[src_p].astype(float), 0, 1)\n",
        "        else:\n",
        "            n2v[\"p_n2v\"] = approx_any(n2v[src_p], n2v[src_n])\n",
        "        n2v = n2v[[\"user_id\",\"p_n2v\"]]\n",
        "else:\n",
        "    n2v = pd.DataFrame({\"user_id\": [], \"p_n2v\": []})\n",
        "\n",
        "# ---- Merge on validation users (from TCN file) ----\n",
        "tcn[\"user_id\"] = tcn[\"user_id\"].astype(str)\n",
        "df = tcn.merge(ft[[\"user_id\",\"p_ft\"]], on=\"user_id\", how=\"left\") \\\n",
        "        .merge(n2v, on=\"user_id\", how=\"left\")\n",
        "df[[\"p_ft\",\"p_n2v\"]] = df[[\"p_ft\",\"p_n2v\"]].fillna(0.0).clip(0,1)\n",
        "\n",
        "legs = [\"proba_tcn\"]\n",
        "if df[\"p_ft\"].notna().any(): legs.append(\"p_ft\")\n",
        "if \"p_n2v\" in df.columns and df[\"p_n2v\"].notna().any(): legs.append(\"p_n2v\")\n",
        "\n",
        "# ---- Metrics per leg (if both classes exist) ----\n",
        "y = df[\"y_true\"].to_numpy(dtype=int)\n",
        "multi_class = (np.unique(y).size == 2)\n",
        "\n",
        "def report(name, p):\n",
        "    au = float(roc_auc_score(y, p)) if multi_class else float(\"nan\")\n",
        "    pr = float(average_precision_score(y, p)) if multi_class else float(\"nan\")\n",
        "    print(f\"{name:<12} AUROC={au:.6f}  PR_AUC={pr:.6f}\")\n",
        "    return au, pr\n",
        "\n",
        "print(\"User-level metrics (validation users):\")\n",
        "for c in legs:\n",
        "    report(c, df[c].to_numpy(float))\n",
        "\n",
        "# ---- Logistic stack (if ≥2 legs AND both classes exist) ----\n",
        "p_stack = None\n",
        "if len(legs) >= 2 and multi_class:\n",
        "    X = df[legs].to_numpy(float)\n",
        "    clf = LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\",\n",
        "                             C=0.5, max_iter=200, random_state=42)\n",
        "    clf.fit(X, y)\n",
        "    p_stack = clf.predict_proba(X)[:,1]\n",
        "    report(\"stack(\" + \"+\".join(legs) + \")\", p_stack)\n",
        "\n",
        "# ---- Rank-average (robust when calibrations differ) ----\n",
        "arr = df[legs].to_numpy(float)\n",
        "ranks = np.argsort(np.argsort(-arr, axis=0), axis=0).astype(float)\n",
        "ranks = 1.0 - (ranks / (len(df)-1))\n",
        "p_rankavg = ranks.mean(axis=1)\n",
        "report(\"rankavg\", p_rankavg)\n",
        "\n",
        "# ---- Save ----\n",
        "out = df[[\"user_id\",\"y_true\"] + legs].copy()\n",
        "if p_stack is not None:\n",
        "    out[\"p_stack\"] = p_stack\n",
        "out[\"p_rankavg\"] = p_rankavg\n",
        "out_path = Path(\"/content/tcn90/user_scores_90d_stack_tcn_ft_n2v.csv\")\n",
        "out.to_csv(out_path, index=False)\n",
        "print(\"Saved →\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XT-fOOoFl_h"
      },
      "source": [
        "# STEP 7H-ROBUST — ΔTPR with min-support + Laplace-smoothed TPR\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y0pkwpGvEuX4"
      },
      "outputs": [],
      "source": [
        "# STEP 7H-ROBUST — ΔTPR with min-support + Laplace-smoothed TPR\n",
        "\n",
        "import numpy as np, pandas as pd, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "STACK  = Path(\"/content/tcn90/user_scores_90d_stack_tcn_ft_n2v.csv\")\n",
        "MERGED = Path(\"/content/merged_trip_details_0.parquet\")\n",
        "assert STACK.exists() and MERGED.exists(), \"Run 7G and Step 2 first.\"\n",
        "\n",
        "# ---- 1) pick score column (prefer p_stack) ----\n",
        "scores = pd.read_csv(STACK)\n",
        "scores[\"user_id\"] = scores[\"user_id\"].astype(str)\n",
        "score_col = next(c for c in [\"p_stack\",\"p_rankavg\",\"proba_tcn\"] if c in scores.columns)\n",
        "\n",
        "# ---- 2) validation weeks from 7F ----\n",
        "with open(\"/content/tcn90/label_info.json\") as f:\n",
        "    info = json.load(f)\n",
        "val_weeks = pd.to_datetime(info.get(\"val_weeks\", []))\n",
        "\n",
        "# ---- 3) choose grouping column & build majority group per user from MERGED ----\n",
        "group_col = \"route_type\"   # change to \"payment_method\"/\"currency\"/\"day_of_week\" if you prefer\n",
        "need_cols = [\"user_id\", group_col, \"trip_start_time\", \"trip_date\"]\n",
        "avail = pd.read_parquet(MERGED, engine=\"pyarrow\").columns\n",
        "use = [c for c in need_cols if c in avail]\n",
        "m = pd.read_parquet(MERGED, engine=\"pyarrow\", columns=use)\n",
        "m[\"user_id\"] = m[\"user_id\"].astype(\"string\")\n",
        "\n",
        "# timestamp → week filter\n",
        "if \"trip_start_time\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_start_time\"], errors=\"coerce\")\n",
        "elif \"trip_date\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_date\"], errors=\"coerce\")\n",
        "else:\n",
        "    ts = None\n",
        "if ts is not None:\n",
        "    m[\"week_start\"] = ts.dt.to_period(\"W-MON\").apply(lambda p: p.start_time)\n",
        "    if len(val_weeks) > 0:\n",
        "        m = m[m[\"week_start\"].isin(val_weeks)]\n",
        "\n",
        "if group_col not in m.columns:\n",
        "    m[group_col] = \"Unknown\"\n",
        "m[group_col] = m[group_col].astype(\"string\").fillna(\"Unknown\")\n",
        "\n",
        "# majority group per user\n",
        "mode_df = (m.groupby(\"user_id\", observed=True)[group_col]\n",
        "             .agg(lambda s: s.dropna().mode().iloc[0] if len(s.dropna()) else \"Unknown\")\n",
        "             .rename(\"group\").reset_index())\n",
        "mode_df[\"user_id\"] = mode_df[\"user_id\"].astype(str)\n",
        "fair = scores.merge(mode_df, on=\"user_id\", how=\"left\")\n",
        "fair[\"group\"] = fair[\"group\"].fillna(\"Unknown\").astype(str)\n",
        "\n",
        "# ---- 4) global threshold chosen to maximize F1 ----\n",
        "y = fair[\"y_true\"].to_numpy(int)\n",
        "p = fair[score_col].to_numpy(float)\n",
        "grid = np.linspace(0.05, 0.95, 19)\n",
        "thr  = grid[np.argmax([f1_score(y, (p>=t).astype(int), zero_division=0) for t in grid])]\n",
        "\n",
        "# ---- 5) robust per-group TPRs: min-support + Laplace smoothing ----\n",
        "MIN_USERS = 200   # tune if needed\n",
        "MIN_POS   = 20\n",
        "\n",
        "rows = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg   = sub[\"y_true\"].to_numpy(int)\n",
        "    pred = (sub[score_col].to_numpy(float) >= thr).astype(int)\n",
        "    n    = len(sub)\n",
        "    pos  = int((yg==1).sum())\n",
        "    tp   = int(((pred==1) & (yg==1)).sum())\n",
        "    # raw TPR and Laplace-smoothed TPR\n",
        "    tpr_raw = (tp/pos) if pos>0 else np.nan\n",
        "    tpr_sm  = (tp + 1) / (pos + 2) if pos>0 else np.nan\n",
        "    rows.append({\"group\": g, \"n_users\": n, \"positives\": pos, \"TPR_raw\": tpr_raw, \"TPR_sm\": tpr_sm})\n",
        "res = pd.DataFrame(rows).sort_values(\"n_users\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "supported = res[(res[\"n_users\"]>=MIN_USERS) & (res[\"positives\"]>=MIN_POS)].copy()\n",
        "delta_raw = float(supported[\"TPR_raw\"].max() - supported[\"TPR_raw\"].min()) if supported[\"TPR_raw\"].notna().any() else float(\"nan\")\n",
        "delta_sm  = float(supported[\"TPR_sm\"].max()  - supported[\"TPR_sm\"].min())  if supported[\"TPR_sm\"].notna().any()  else float(\"nan\")\n",
        "\n",
        "out = Path(f\"/content/tcn90/fairness_{group_col}_delta_tpr_robust.csv\")\n",
        "res.to_csv(out, index=False)\n",
        "print(f\"Using score: {score_col}, threshold={float(thr)}\")\n",
        "print(f\"Min-support: users>={MIN_USERS}, positives>={MIN_POS}\")\n",
        "print(\"ΔTPR_raw (supported):\", delta_raw)\n",
        "print(\"ΔTPR_sm  (supported):\",  delta_sm)\n",
        "print(\"Saved →\", out)\n",
        "print(res.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpWQ2t1aT-D2"
      },
      "source": [
        "# STEP 7H-EO-THRESH-ROBUST — Equal Opportunity by route_type with min-support + per-group thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZHK3abZnT3YW"
      },
      "outputs": [],
      "source": [
        "# STEP 7H-EO-THRESH-ROBUST — Equal Opportunity by route_type with min-support + per-group thresholds\n",
        "\n",
        "import numpy as np, pandas as pd, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "STACK  = Path(\"/content/tcn90/user_scores_90d_stack_tcn_ft_n2v.csv\")\n",
        "MERGED = Path(\"/content/merged_trip_details_0.parquet\")\n",
        "assert STACK.exists() and MERGED.exists(), \"Need 7G stack + merged parquet.\"\n",
        "\n",
        "# 1) Load stacked scores and pick score column (prefer p_stack)\n",
        "scores = pd.read_csv(STACK)\n",
        "scores[\"user_id\"] = scores[\"user_id\"].astype(str)\n",
        "score_col = next(c for c in [\"p_stack\",\"p_rankavg\",\"proba_tcn\"] if c in scores.columns)\n",
        "\n",
        "# 2) Validation weeks (align fairness window to your TCN val horizon)\n",
        "with open(\"/content/tcn90/label_info.json\") as f:\n",
        "    info = json.load(f)\n",
        "val_weeks = pd.to_datetime(info.get(\"val_weeks\", []))\n",
        "\n",
        "# 3) Build route_type per user from MERGED (majority during validation weeks)\n",
        "need = [\"user_id\",\"route_type\",\"trip_start_time\",\"trip_date\"]\n",
        "avail = pd.read_parquet(MERGED, engine=\"pyarrow\").columns\n",
        "use = [c for c in need if c in avail]\n",
        "m = pd.read_parquet(MERGED, engine=\"pyarrow\", columns=use)\n",
        "m[\"user_id\"] = m[\"user_id\"].astype(\"string\")\n",
        "\n",
        "# timestamp → week filter\n",
        "if \"trip_start_time\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_start_time\"], errors=\"coerce\")\n",
        "elif \"trip_date\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_date\"], errors=\"coerce\")\n",
        "else:\n",
        "    ts = None\n",
        "if ts is not None:\n",
        "    # strip tz before to_period (avoid warning)\n",
        "    if getattr(ts.dtype, \"tz\", None) is not None:\n",
        "        ts = ts.dt.tz_localize(None)\n",
        "    m[\"week_start\"] = ts.dt.to_period(\"W-MON\").apply(lambda p: p.start_time)\n",
        "    if len(val_weeks) > 0:\n",
        "        m = m[m[\"week_start\"].isin(val_weeks)]\n",
        "\n",
        "if \"route_type\" not in m.columns:\n",
        "    m[\"route_type\"] = \"Unknown\"\n",
        "m[\"route_type\"] = m[\"route_type\"].astype(\"string\").fillna(\"Unknown\")\n",
        "\n",
        "mode_df = (m.groupby(\"user_id\", observed=True)[\"route_type\"]\n",
        "             .agg(lambda s: s.dropna().mode().iloc[0] if len(s.dropna()) else \"Unknown\")\n",
        "             .rename(\"group\").reset_index())\n",
        "mode_df[\"user_id\"] = mode_df[\"user_id\"].astype(str)\n",
        "\n",
        "fair = scores.merge(mode_df, on=\"user_id\", how=\"left\").fillna({\"group\":\"Unknown\"})\n",
        "y = fair[\"y_true\"].to_numpy(int)\n",
        "p = fair[score_col].to_numpy(float)\n",
        "\n",
        "# 4) Global threshold (F1-opt on all users)\n",
        "grid = np.linspace(0.05, 0.95, 19)\n",
        "thr_global = grid[np.argmax([f1_score(y, (p>=t).astype(int), zero_division=0) for t in grid])]\n",
        "\n",
        "# 5) Collapse undersized groups into 'Other' (min support)\n",
        "MIN_USERS, MIN_POS = 200, 20\n",
        "stats0 = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int)\n",
        "    stats0.append({\"group\": g, \"n\": len(sub), \"pos\": int((yg==1).sum())})\n",
        "stats0 = pd.DataFrame(stats0)\n",
        "small = set(stats0[(stats0[\"n\"]<MIN_USERS) | (stats0[\"pos\"]<MIN_POS)][\"group\"].tolist())\n",
        "fair[\"group\"] = fair[\"group\"].apply(lambda g: \"Other\" if g in small else g)\n",
        "\n",
        "# 6) Target TPR = median TPR among supported groups at global threshold\n",
        "def tpr_at(y, pred):\n",
        "    pos = int((y==1).sum())\n",
        "    return (int(((pred==1)&(y==1)).sum())/pos) if pos>0 else np.nan\n",
        "\n",
        "stats = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pg = sub[score_col].to_numpy(float)\n",
        "    stats.append({\"group\": g, \"n\": len(sub), \"pos\": int((yg==1).sum()),\n",
        "                  \"tpr_global\": tpr_at(yg, (pg>=thr_global).astype(int))})\n",
        "stats = pd.DataFrame(stats).sort_values(\"n\", ascending=False)\n",
        "supported = stats[(stats[\"n\"]>=MIN_USERS) & (stats[\"pos\"]>=MIN_POS)].copy()\n",
        "target_tpr = float(np.nanmedian(supported[\"tpr_global\"])) if supported[\"tpr_global\"].notna().any() else float(\"nan\")\n",
        "\n",
        "# 7) Per-group threshold to match target TPR (tie-break by in-group F1)\n",
        "def best_thr_for_group(yg, pg, target, grid, thr_fallback):\n",
        "    if np.isnan(target) or len(yg)==0 or yg.sum()==0:\n",
        "        return thr_fallback\n",
        "    best_t, best_gap, best_f1 = thr_fallback, 1e9, -1\n",
        "    for t in grid:\n",
        "        pred = (pg>=t).astype(int)\n",
        "        pos = int((yg==1).sum())\n",
        "        tpr = (int(((pred==1)&(yg==1)).sum())/pos) if pos>0 else np.nan\n",
        "        gap = abs((tpr if not np.isnan(tpr) else 0) - target)\n",
        "        f1  = f1_score(yg, pred, zero_division=0)\n",
        "        if gap < best_gap - 1e-6 or (abs(gap-best_gap)<=1e-6 and f1>best_f1):\n",
        "            best_t, best_gap, best_f1 = t, gap, f1\n",
        "    return best_t\n",
        "\n",
        "thr_map = {}\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pg = sub[score_col].to_numpy(float)\n",
        "    thr_map[g] = best_thr_for_group(yg, pg, target_tpr, grid, thr_global)\n",
        "\n",
        "# 8) Apply EO thresholds and measure ΔTPR after equalization\n",
        "pred_equal = np.zeros(len(fair), dtype=int)\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    t = thr_map[g]\n",
        "    idx = sub.index\n",
        "    pred_equal[idx] = (sub[score_col].to_numpy(float) >= t).astype(int)\n",
        "\n",
        "rows = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pe = pred_equal[sub.index]\n",
        "    pos = int((yg==1).sum())\n",
        "    tpr = (int(((pe==1)&(yg==1)).sum())/pos) if pos>0 else np.nan\n",
        "    rows.append({\"group\": g, \"n\": len(sub), \"pos\": pos, \"TPR_after_EO\": tpr, \"thr\": thr_map[g]})\n",
        "eq = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
        "delta_after = float(eq[\"TPR_after_EO\"].max() - eq[\"TPR_after_EO\"].min()) if eq[\"TPR_after_EO\"].notna().any() else float(\"nan\")\n",
        "\n",
        "# 9) Save artifacts\n",
        "Path(\"/content/tcn90\").mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame({\"group\": list(thr_map.keys()), \"threshold\": list(thr_map.values())}) \\\n",
        "  .to_csv(\"/content/tcn90/eo_thresholds_by_group.csv\", index=False)\n",
        "out = fair[[\"user_id\",\"group\",score_col,\"y_true\"]].copy()\n",
        "out[\"pred_equalized\"] = pred_equal\n",
        "out.to_csv(\"/content/tcn90/user_equalized_preds.csv\", index=False)\n",
        "\n",
        "print(\"Global thr:\", float(thr_global), \"| target TPR:\", target_tpr)\n",
        "print(\"ΔTPR after EO (max–min):\", delta_after)\n",
        "print(\"Saved → /content/tcn90/eo_thresholds_by_group.csv\")\n",
        "print(\"Saved → /content/tcn90/user_equalized_preds.csv\")\n",
        "print(eq.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdUI8QFQV7Ou"
      },
      "source": [
        "# STEP 7H-EO-THRESH-ROBUST — Equal Opportunity by route_type with min-support + per-group thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WjIJWK5DV3da"
      },
      "outputs": [],
      "source": [
        "# STEP 7H-EO-THRESH-ROBUST — Equal Opportunity by route_type with min-support + per-group thresholds\n",
        "\n",
        "import numpy as np, pandas as pd, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "STACK  = Path(\"/content/tcn90/user_scores_90d_stack_tcn_ft_n2v.csv\")\n",
        "MERGED = Path(\"/content/merged_trip_details_0.parquet\")\n",
        "assert STACK.exists() and MERGED.exists(), \"Need 7G stack + merged parquet.\"\n",
        "\n",
        "# 1) Load stacked scores and pick score column (prefer p_stack)\n",
        "scores = pd.read_csv(STACK)\n",
        "scores[\"user_id\"] = scores[\"user_id\"].astype(str)\n",
        "score_col = next(c for c in [\"p_stack\",\"p_rankavg\",\"proba_tcn\"] if c in scores.columns)\n",
        "\n",
        "# 2) Validation weeks (align fairness window to your TCN val horizon)\n",
        "with open(\"/content/tcn90/label_info.json\") as f:\n",
        "    info = json.load(f)\n",
        "val_weeks = pd.to_datetime(info.get(\"val_weeks\", []))\n",
        "\n",
        "# 3) Build route_type per user from MERGED (majority during validation weeks)\n",
        "need = [\"user_id\",\"route_type\",\"trip_start_time\",\"trip_date\"]\n",
        "avail = pd.read_parquet(MERGED, engine=\"pyarrow\").columns\n",
        "use = [c for c in need if c in avail]\n",
        "m = pd.read_parquet(MERGED, engine=\"pyarrow\", columns=use)\n",
        "m[\"user_id\"] = m[\"user_id\"].astype(\"string\")\n",
        "\n",
        "# timestamp → week filter\n",
        "if \"trip_start_time\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_start_time\"], errors=\"coerce\")\n",
        "elif \"trip_date\" in m.columns:\n",
        "    ts = pd.to_datetime(m[\"trip_date\"], errors=\"coerce\")\n",
        "else:\n",
        "    ts = None\n",
        "if ts is not None:\n",
        "    # strip tz before to_period (avoid warning)\n",
        "    if getattr(ts.dtype, \"tz\", None) is not None:\n",
        "        ts = ts.dt.tz_localize(None)\n",
        "    m[\"week_start\"] = ts.dt.to_period(\"W-MON\").apply(lambda p: p.start_time)\n",
        "    if len(val_weeks) > 0:\n",
        "        m = m[m[\"week_start\"].isin(val_weeks)]\n",
        "\n",
        "if \"route_type\" not in m.columns:\n",
        "    m[\"route_type\"] = \"Unknown\"\n",
        "m[\"route_type\"] = m[\"route_type\"].astype(\"string\").fillna(\"Unknown\")\n",
        "\n",
        "mode_df = (m.groupby(\"user_id\", observed=True)[\"route_type\"]\n",
        "             .agg(lambda s: s.dropna().mode().iloc[0] if len(s.dropna()) else \"Unknown\")\n",
        "             .rename(\"group\").reset_index())\n",
        "mode_df[\"user_id\"] = mode_df[\"user_id\"].astype(str)\n",
        "\n",
        "fair = scores.merge(mode_df, on=\"user_id\", how=\"left\").fillna({\"group\":\"Unknown\"})\n",
        "y = fair[\"y_true\"].to_numpy(int)\n",
        "p = fair[score_col].to_numpy(float)\n",
        "\n",
        "# 4) Global threshold (F1-opt on all users)\n",
        "grid = np.linspace(0.05, 0.95, 19)\n",
        "thr_global = grid[np.argmax([f1_score(y, (p>=t).astype(int), zero_division=0) for t in grid])]\n",
        "\n",
        "# 5) Collapse undersized groups into 'Other' (min support)\n",
        "MIN_USERS, MIN_POS = 200, 20\n",
        "stats0 = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int)\n",
        "    stats0.append({\"group\": g, \"n\": len(sub), \"pos\": int((yg==1).sum())})\n",
        "stats0 = pd.DataFrame(stats0)\n",
        "small = set(stats0[(stats0[\"n\"]<MIN_USERS) | (stats0[\"pos\"]<MIN_POS)][\"group\"].tolist())\n",
        "fair[\"group\"] = fair[\"group\"].apply(lambda g: \"Other\" if g in small else g)\n",
        "\n",
        "# 6) Target TPR = median TPR among supported groups at global threshold\n",
        "def tpr_at(y, pred):\n",
        "    pos = int((y==1).sum())\n",
        "    return (int(((pred==1)&(y==1)).sum())/pos) if pos>0 else np.nan\n",
        "\n",
        "stats = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pg = sub[score_col].to_numpy(float)\n",
        "    stats.append({\"group\": g, \"n\": len(sub), \"pos\": int((yg==1).sum()),\n",
        "                  \"tpr_global\": tpr_at(yg, (pg>=thr_global).astype(int))})\n",
        "stats = pd.DataFrame(stats).sort_values(\"n\", ascending=False)\n",
        "supported = stats[(stats[\"n\"]>=MIN_USERS) & (stats[\"pos\"]>=MIN_POS)].copy()\n",
        "target_tpr = float(np.nanmedian(supported[\"tpr_global\"])) if supported[\"tpr_global\"].notna().any() else float(\"nan\")\n",
        "\n",
        "# 7) Per-group threshold to match target TPR (tie-break by in-group F1)\n",
        "def best_thr_for_group(yg, pg, target, grid, thr_fallback):\n",
        "    if np.isnan(target) or len(yg)==0 or yg.sum()==0:\n",
        "        return thr_fallback\n",
        "    best_t, best_gap, best_f1 = thr_fallback, 1e9, -1\n",
        "    for t in grid:\n",
        "        pred = (pg>=t).astype(int)\n",
        "        pos = int((yg==1).sum())\n",
        "        tpr = (int(((pred==1)&(yg==1)).sum())/pos) if pos>0 else np.nan\n",
        "        gap = abs((tpr if not np.isnan(tpr) else 0) - target)\n",
        "        f1  = f1_score(yg, pred, zero_division=0)\n",
        "        if gap < best_gap - 1e-6 or (abs(gap-best_gap)<=1e-6 and f1>best_f1):\n",
        "            best_t, best_gap, best_f1 = t, gap, f1\n",
        "    return best_t\n",
        "\n",
        "thr_map = {}\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pg = sub[score_col].to_numpy(float)\n",
        "    thr_map[g] = best_thr_for_group(yg, pg, target_tpr, grid, thr_global)\n",
        "\n",
        "# 8) Apply EO thresholds and measure ΔTPR after equalization\n",
        "pred_equal = np.zeros(len(fair), dtype=int)\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    t = thr_map[g]\n",
        "    idx = sub.index\n",
        "    pred_equal[idx] = (sub[score_col].to_numpy(float) >= t).astype(int)\n",
        "\n",
        "rows = []\n",
        "for g, sub in fair.groupby(\"group\"):\n",
        "    yg = sub[\"y_true\"].to_numpy(int); pe = pred_equal[sub.index]\n",
        "    pos = int((yg==1).sum())\n",
        "    tpr = (int(((pe==1)&(yg==1)).sum())/pos) if pos>0 else np.nan\n",
        "    rows.append({\"group\": g, \"n\": len(sub), \"pos\": pos, \"TPR_after_EO\": tpr, \"thr\": thr_map[g]})\n",
        "eq = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
        "delta_after = float(eq[\"TPR_after_EO\"].max() - eq[\"TPR_after_EO\"].min()) if eq[\"TPR_after_EO\"].notna().any() else float(\"nan\")\n",
        "\n",
        "# 9) Save artifacts\n",
        "Path(\"/content/tcn90\").mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame({\"group\": list(thr_map.keys()), \"threshold\": list(thr_map.values())}) \\\n",
        "  .to_csv(\"/content/tcn90/eo_thresholds_by_group.csv\", index=False)\n",
        "out = fair[[\"user_id\",\"group\",score_col,\"y_true\"]].copy()\n",
        "out[\"pred_equalized\"] = pred_equal\n",
        "out.to_csv(\"/content/tcn90/user_equalized_preds.csv\", index=False)\n",
        "\n",
        "print(\"Global thr:\", float(thr_global), \"| target TPR:\", target_tpr)\n",
        "print(\"ΔTPR after EO (max–min):\", delta_after)\n",
        "print(\"Saved → /content/tcn90/eo_thresholds_by_group.csv\")\n",
        "print(\"Saved → /content/tcn90/user_equalized_preds.csv\")\n",
        "print(eq.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhqCu5VQvqN4"
      },
      "source": [
        "# STEP 7D-ALT — data-driven stretch to 100–900 (no fixed buckets, preserves ranking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xKNk7x-yvJSJ"
      },
      "outputs": [],
      "source": [
        "# STEP 7D — Robust percentile scaling to Nova 350–900 (no edge spikes)\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "\n",
        "# 1) Pick source (prefers ensemble 90d)\n",
        "candidates = [\n",
        "    \"/content/ensemble_user_scores_90d.csv\",   # from Step 7C\n",
        "    \"/content/ft90/user_scores_90d_ft.csv\",\n",
        "    \"/content/user_scores_90d_n2v_cal.csv\",\n",
        "]\n",
        "path = next((p for p in candidates if os.path.exists(p)), None)\n",
        "assert path is not None, \"No score file found. Run Step 7C (ensemble) or earlier scoring first.\"\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# 2) Get one risk column (higher = riskier)\n",
        "if \"proba_blend\" in df.columns:\n",
        "    risk = pd.to_numeric(df[\"proba_blend\"], errors=\"coerce\").to_numpy()\n",
        "elif \"proba_mean\" in df.columns:\n",
        "    risk = pd.to_numeric(df[\"proba_mean\"], errors=\"coerce\").to_numpy()\n",
        "elif \"risk_proba\" in df.columns:\n",
        "    risk = pd.to_numeric(df[\"risk_proba\"], errors=\"coerce\").to_numpy()\n",
        "else:\n",
        "    raise ValueError(\"Need one of: proba_blend / proba_mean / risk_proba\")\n",
        "\n",
        "mask = np.isfinite(risk)\n",
        "df = df.loc[mask].copy()\n",
        "risk = risk[mask]\n",
        "\n",
        "# 3) Robust percentile scaling (keeps real shape, avoids edge piles)\n",
        "#    Only the central 98% of risk is mapped linearly; extreme 2% are clipped.\n",
        "p_lo, p_hi = np.quantile(risk, [0.02, 0.98])\n",
        "z = (risk - p_lo) / max(1e-9, (p_hi - p_lo))\n",
        "z = np.clip(z, 0.0, 1.0)\n",
        "\n",
        "# Optional gentle curvature (gamma ~ 1.0 keeps near-linear)\n",
        "gamma = 1.05   # >1 compresses extremes slightly; set =1.0 for pure linear\n",
        "z_adj = z**gamma\n",
        "\n",
        "# 4) Map to Nova 350–900 (higher risk => lower score)\n",
        "low, high = 350.0, 900.0\n",
        "df[\"nova_score\"] = (high - (high - low) * z_adj).astype(\"float32\")\n",
        "\n",
        "# 5) Decision bands on 350–900 scale\n",
        "edges  = [350, 500, 600, 650, 700, 750, 800, 850, 900]\n",
        "labels = [\"<500\",\"500–599\",\"600–649\",\"650–699\",\"700–749\",\"750–799\",\"800–849\",\"≥850\"]\n",
        "df[\"decision_band\"] = pd.cut(df[\"nova_score\"], bins=edges, right=False,\n",
        "                             labels=labels, include_lowest=True)\n",
        "\n",
        "# Keep useful columns\n",
        "keep = [\"user_id\",\"nova_score\",\"decision_band\"]\n",
        "for extra in [\"trips\",\"risk_proba\",\"proba_blend\",\"proba_mean\"]:\n",
        "    if extra in df.columns: keep.append(extra)\n",
        "\n",
        "OUT = \"/content/nova_scores_90d_final_scaled_350_900.csv\"\n",
        "df[keep].to_csv(OUT, index=False)\n",
        "print(f\"Saved → {OUT}\")\n",
        "print(f\"Robust bounds used: p_lo={p_lo:.4f}, p_hi={p_hi:.4f}  (gamma={gamma})\")\n",
        "print(df.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE5adMpDG9w2"
      },
      "source": [
        "# STEP 7D-DIST-FIX — robust band distribution for 90d (uses stretched if present)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j4h36w4IG9Fs"
      },
      "outputs": [],
      "source": [
        "# STEP 7D-DIST-FIX (350–900) — robust band distribution\n",
        "\n",
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "candidates = [\n",
        "    \"/content/nova_scores_90d_final_scaled_350_900.csv\",  # ← new file\n",
        "    \"/content/nova_scores_90d_final_widespread.csv\",\n",
        "    \"/content/nova_scores_90d_final.csv\",\n",
        "]\n",
        "OUT = next((p for p in candidates if os.path.exists(p)), None)\n",
        "assert OUT is not None, \"Run Step 7D first.\"\n",
        "\n",
        "df = pd.read_csv(OUT)\n",
        "scores = pd.to_numeric(df[\"nova_score\"], errors=\"coerce\").dropna().to_numpy()\n",
        "\n",
        "edges  = [350, 500, 600, 650, 700, 750, 800, 850, 900]\n",
        "labels = [\"<500\",\"500–599\",\"600–649\",\"650–699\",\"700–749\",\"750–799\",\"800–849\",\"≥850\"]\n",
        "\n",
        "counts, _ = np.histogram(scores, bins=edges)\n",
        "dist = pd.DataFrame({\"nova_score\": labels, \"count\": counts})\n",
        "dist[\"percent\"] = (dist[\"count\"] / max(1, dist[\"count\"].sum()) * 100).round(2)\n",
        "\n",
        "print(\"Distribution (Nova 350–900):\")\n",
        "print(dist.to_string(index=False))\n",
        "dist.to_csv(\"/content/nova_scores_90d_band_dist.csv\", index=False)\n",
        "print(\"\\nSaved → /content/nova_scores_90d_band_dist.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so83VNCfwXXX"
      },
      "source": [
        "# STEP 8A — Export final 90d outputs to Google Drive (with stretched 100–900 file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eWm-NKMSvvM7"
      },
      "outputs": [],
      "source": [
        "# STEP 8A — Export final 90d outputs to Google Drive (Nova 350–900)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, shutil, glob, zipfile\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/NovaScore_Outputs_90d\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "CANDIDATES = [\n",
        "    # Final decision files (prefer new 350–900)\n",
        "    \"/content/nova_scores_90d_final_scaled_350_900.csv\",   # ← NEW\n",
        "    \"/content/nova_scores_90d_final_widespread.csv\",\n",
        "    \"/content/nova_scores_90d_final.csv\",\n",
        "    \"/content/nova_scores_90d_band_dist.csv\",\n",
        "\n",
        "    # User-level scores / previews\n",
        "    \"/content/ensemble_user_scores_90d.csv\",\n",
        "    \"/content/ft90/user_scores_90d_ft.csv\",\n",
        "    \"/content/user_scores_90d_n2v.csv\",\n",
        "    \"/content/user_scores_90d_n2v_cal.csv\",\n",
        "    \"/content/val_preds_90d_ft_sample.csv\",\n",
        "    \"/content/val_preds_90d_n2v_sample.csv\",\n",
        "\n",
        "    # FT artifacts (90d)\n",
        "    \"/content/ft90/ft_model.pt\",\n",
        "    \"/content/ft90/ft_val_proba.npy\",\n",
        "    \"/content/ft90/val_ids.parquet\",\n",
        "    \"/content/ft90/num_cols.json\",\n",
        "    \"/content/ft90/mu.npy\",\n",
        "    \"/content/ft90/sd.npy\",\n",
        "]\n",
        "\n",
        "# Also include any extra shaped/stretched files if present\n",
        "CANDIDATES += glob.glob(\"/content/*90d*shaped*.csv\")\n",
        "CANDIDATES += glob.glob(\"/content/*stretched*90d*.csv\")\n",
        "\n",
        "copied, missing = [], []\n",
        "for src in CANDIDATES:\n",
        "    if os.path.exists(src):\n",
        "        if \"/content/ft90/\" in src:\n",
        "            subdir = os.path.join(OUT_DIR, \"ft90\")\n",
        "            os.makedirs(subdir, exist_ok=True)\n",
        "            dst = os.path.join(subdir, os.path.basename(src))\n",
        "        else:\n",
        "            dst = os.path.join(OUT_DIR, os.path.basename(src))\n",
        "        shutil.copy2(src, dst); copied.append(dst)\n",
        "    else:\n",
        "        missing.append(src)\n",
        "\n",
        "# Optional: zip the entire ft90 folder too\n",
        "FT_DIR = \"/content/ft90\"\n",
        "if os.path.isdir(FT_DIR):\n",
        "    zip_path = \"/content/ft90_artifacts.zip\"\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root, _, files in os.walk(FT_DIR):\n",
        "            for f in files:\n",
        "                full = os.path.join(root, f)\n",
        "                rel  = os.path.relpath(full, \"/content\")\n",
        "                zf.write(full, rel)\n",
        "    shutil.copy2(zip_path, os.path.join(OUT_DIR, os.path.basename(zip_path)))\n",
        "    copied.append(os.path.join(OUT_DIR, os.path.basename(zip_path)))\n",
        "\n",
        "print(\"✅ Export complete.\")\n",
        "print(f\"Saved to Drive folder: {OUT_DIR}\\n\")\n",
        "print(\"Files copied:\")\n",
        "for p in copied: print(\"  -\", p)\n",
        "\n",
        "if missing:\n",
        "    print(\"\\n(Skipped — not found):\")\n",
        "    for m in missing: print(\"  -\", m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQpVVhcYyu0_"
      },
      "source": [
        "# STEP 8B — Probe runtime + Drive for 90d outputs, zip what exists, and download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_6aBbCZHwbIf"
      },
      "outputs": [],
      "source": [
        "# STEP 8B — Probe runtime + Drive for 90d outputs, zip what exists, and download\n",
        "\n",
        "import os, zipfile, fnmatch\n",
        "from google.colab import drive, files\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "TARGET_NAMES = [\n",
        "    \"nova_scores_90d_final_scaled_350_900.csv\",  # ← preferred 350–900 file\n",
        "    \"nova_scores_90d_final.csv\",\n",
        "    \"nova_scores_90d_final_widespread.csv\",\n",
        "    \"nova_scores_90d_band_dist.csv\",\n",
        "    \"ensemble_user_scores_90d.csv\",\n",
        "    \"user_scores_90d_ft.csv\",\n",
        "    \"user_scores_90d_n2v.csv\",\n",
        "    \"user_scores_90d_n2v_cal.csv\",\n",
        "    \"val_preds_90d_ft_sample.csv\",\n",
        "    \"val_preds_90d_n2v_sample.csv\",\n",
        "    \"ft_model.pt\", \"ft_val_proba.npy\", \"val_ids.parquet\"\n",
        "]\n",
        "\n",
        "SEARCH_ROOTS = [\"/content\", \"/content/drive/MyDrive\"]\n",
        "SKIP_DIRS = {\".Trash\", \".config\", \".ipynb_checkpoints\", \".cache\"}\n",
        "\n",
        "found = {}\n",
        "def scan_for_targets(root):\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        dirnames[:] = [d for d in dirnames if d not in SKIP_DIRS and not d.startswith(\".\")]\n",
        "        for fn in filenames:\n",
        "            if fn in TARGET_NAMES and fn not in found:\n",
        "                p = os.path.join(dirpath, fn)\n",
        "                try:\n",
        "                    if os.path.getsize(p) > 0:\n",
        "                        found[fn] = p\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "for r in SEARCH_ROOTS:\n",
        "    if os.path.isdir(r):\n",
        "        scan_for_targets(r)\n",
        "\n",
        "if not found:\n",
        "    print(\"⚠️ No exact-name matches; listing likely candidates...\\n\")\n",
        "    candidates, PATTERNS = [], [\"*scaled*350*900*csv\", \"*90d*csv\", \"*ensemble*90d*csv\", \"*val*_preds*90d*csv\"]\n",
        "    for r in SEARCH_ROOTS:\n",
        "        for dirpath, dirnames, filenames in os.walk(r):\n",
        "            dirnames[:] = [d for d in dirnames if d not in SKIP_DIRS and not d.startswith(\".\")]\n",
        "            for fn in filenames:\n",
        "                fpath = os.path.join(dirpath, fn)\n",
        "                try:\n",
        "                    if os.path.getsize(fpath) == 0: continue\n",
        "                except Exception:\n",
        "                    continue\n",
        "                for pat in PATTERNS:\n",
        "                    if fnmatch.fnmatch(fn.lower(), pat.lower()):\n",
        "                        candidates.append(fpath); break\n",
        "    for p in sorted(candidates)[:100]: print(\" \", p)\n",
        "else:\n",
        "    # Prefer 90d FT artifacts if present\n",
        "    for name, desired in {\n",
        "        \"ft_model.pt\": \"/content/ft90/ft_model.pt\",\n",
        "        \"ft_val_proba.npy\": \"/content/ft90/ft_val_proba.npy\",\n",
        "    }.items():\n",
        "        if os.path.exists(desired): found[name] = desired\n",
        "\n",
        "    bundle = \"/content/NovaScore_90d_results_bundle.zip\"\n",
        "    with zipfile.ZipFile(bundle, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for name, path in found.items():\n",
        "            arcname = path.replace(\"/content/drive/MyDrive/\", \"MyDrive/\").replace(\"/content/\", \"\")\n",
        "            zf.write(path, arcname)\n",
        "    print(\"✅ Bundled files:\")\n",
        "    for k, v in found.items(): print(f\"  - {k}: {v}\")\n",
        "    print(\"\\nDownloading ZIP...\")\n",
        "    files.download(bundle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3fStIvXlPan"
      },
      "source": [
        "# STEP 8C — Save Nova-score visuals (90d) to /content/plots_nova_90d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-DUhO964CwbJ"
      },
      "outputs": [],
      "source": [
        "# STEP 8C — Save Nova-score visuals (90d) to /content/plots_nova_90d (Nova 350–900)\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from datetime import datetime\n",
        "\n",
        "candidates = [\n",
        "    \"/content/nova_scores_90d_final_scaled_350_900.csv\",  # ← prefer this\n",
        "    \"/content/ensemble_user_scores_90d.csv\",\n",
        "    \"/content/ft90/user_scores_90d_ft.csv\",\n",
        "    \"/content/user_scores_90d_n2v_cal.csv\",\n",
        "    \"/content/nova_scores_90d_final_widespread.csv\",\n",
        "    \"/content/nova_scores_90d_final.csv\",\n",
        "]\n",
        "PATH = next((p for p in candidates if os.path.exists(p)), None)\n",
        "assert PATH, \"No score file found.\"\n",
        "\n",
        "df = pd.read_csv(PATH)\n",
        "if \"nova_score\" not in df.columns:\n",
        "    if \"risk_proba\" in df.columns:\n",
        "        df[\"nova_score\"] = 900 - 600 * pd.to_numeric(df[\"risk_proba\"], errors=\"coerce\")\n",
        "    else:\n",
        "        raise AssertionError(\"No 'nova_score' or 'risk_proba'.\")\n",
        "df[\"nova_score\"] = pd.to_numeric(df[\"nova_score\"], errors=\"coerce\").clip(350, 900)\n",
        "df = df.dropna(subset=[\"nova_score\"]).copy()\n",
        "\n",
        "bg, fg, grid_c, accent = \"#f6fbf6\", \"#1f2937\", \"#e3eee3\", \"#2f855a\"\n",
        "mpl.rcParams.update({\n",
        "    \"figure.facecolor\": bg, \"axes.facecolor\": bg, \"axes.edgecolor\": \"none\",\n",
        "    \"axes.labelcolor\": fg, \"xtick.color\": fg, \"ytick.color\": fg,\n",
        "    \"grid.color\": grid_c, \"grid.linestyle\": \"-\", \"grid.linewidth\": 0.8,\n",
        "    \"axes.grid\": True, \"axes.grid.axis\": \"y\", \"font.size\": 11,\n",
        "})\n",
        "greens = mpl.cm.Greens\n",
        "\n",
        "out_dir = \"/content/plots_nova_90d\"; os.makedirs(out_dir, exist_ok=True)\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Histogram\n",
        "plt.figure(figsize=(9,4.8))\n",
        "bins = np.linspace(df[\"nova_score\"].min(), df[\"nova_score\"].max(), 30)\n",
        "counts, edges, patches = plt.hist(df[\"nova_score\"].values, bins=bins, edgecolor=\"white\", linewidth=0.6)\n",
        "norm = mpl.colors.Normalize(vmin=max(1, counts.min() if len(counts) else 1), vmax=max(1, counts.max() if len(counts) else 1))\n",
        "for c, p in zip(counts, patches): p.set_facecolor(greens(0.35 + 0.65 * norm(c)))\n",
        "for thr in [500, 600, 650, 700, 750, 800, 850]: plt.axvline(thr, color=accent, linestyle=\"--\", linewidth=1, alpha=0.35)\n",
        "plt.title(f\"Nova score distribution (90d, {len(df):,} users)\", color=fg, pad=10)\n",
        "plt.xlabel(\"Nova score\"); plt.ylabel(\"Users\")\n",
        "plt.tight_layout()\n",
        "hist_png = os.path.join(out_dir, f\"nova_hist_90d_{stamp}.png\")\n",
        "hist_svg = os.path.join(out_dir, f\"nova_hist_90d_{stamp}.svg\")\n",
        "plt.savefig(hist_png, dpi=220, bbox_inches=\"tight\"); plt.savefig(hist_svg, dpi=220, bbox_inches=\"tight\"); plt.show()\n",
        "\n",
        "# Bands (350–900)\n",
        "labels = [\"<500\",\"500–599\",\"600–649\",\"650–699\",\"700–749\",\"750–799\",\"800–849\",\"≥850\"]\n",
        "edges_fixed = [350, 500, 600, 650, 700, 750, 800, 850, 900]\n",
        "counts_b, _ = np.histogram(df[\"nova_score\"].to_numpy(dtype=float), bins=edges_fixed)\n",
        "perc_b = (counts_b / max(1, counts_b.sum()) * 100).round(2)\n",
        "\n",
        "plt.figure(figsize=(9,4.8))\n",
        "colors = greens(np.linspace(0.45, 0.9, len(labels)))\n",
        "bars = plt.bar(labels, counts_b, color=colors, edgecolor=\"white\", linewidth=0.6)\n",
        "for b, p in zip(bars, perc_b):\n",
        "    y = b.get_height(); plt.text(b.get_x()+b.get_width()/2, y, f\"{p:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=10, color=fg)\n",
        "plt.title(\"Users per Nova band (90d)\", color=fg, pad=10)\n",
        "plt.xlabel(\"Band\"); plt.ylabel(\"Users\")\n",
        "plt.tight_layout()\n",
        "bands_png = os.path.join(out_dir, f\"nova_bands_90d_{stamp}.png\")\n",
        "bands_svg = os.path.join(out_dir, f\"nova_bands_90d_{stamp}.svg\")\n",
        "plt.savefig(bands_png, dpi=220, bbox_inches=\"tight\"); plt.savefig(bands_svg, dpi=220, bbox_inches=\"tight\"); plt.show()\n",
        "\n",
        "# CDF\n",
        "plt.figure(figsize=(9,4.8))\n",
        "xs = np.sort(df[\"nova_score\"].values); ys = np.linspace(0, 1, len(xs), endpoint=True)\n",
        "plt.plot(xs, ys, linewidth=2, color=accent); plt.fill_between(xs, ys, alpha=0.08, color=accent)\n",
        "for thr in [500, 600, 650, 700, 750, 800, 850]: plt.axvline(thr, color=accent, linestyle=\"--\", linewidth=1, alpha=0.25)\n",
        "plt.title(\"Cumulative distribution of Nova score (CDF, 90d)\", color=fg, pad=10)\n",
        "plt.xlabel(\"Nova score\"); plt.ylabel(\"Cumulative share of users\")\n",
        "plt.tight_layout()\n",
        "cdf_png = os.path.join(out_dir, f\"nova_cdf_90d_{stamp}.png\")\n",
        "cdf_svg = os.path.join(out_dir, f\"nova_cdf_90d_{stamp}.svg\")\n",
        "plt.savefig(cdf_png, dpi=220, bbox_inches=\"tight\"); plt.savefig(cdf_svg, dpi=220, bbox_inches=\"tight\"); plt.show()\n",
        "\n",
        "print(\"Saved files:\")\n",
        "for p in [hist_png, hist_svg, bands_png, bands_svg, cdf_png, cdf_svg]:\n",
        "    print(\" -\", p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC989VMRG74K"
      },
      "source": [
        "# STEP 8D — Zip the 90d plot images and download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wsF7-dzZlYVE"
      },
      "outputs": [],
      "source": [
        "# STEP 8D — Zip only the selected 90d plot images and download\n",
        "\n",
        "import os, zipfile\n",
        "from google.colab import files\n",
        "\n",
        "FILES = [\n",
        "    \"/content/plots_nova_90d/nova_hist_90d_20250907-132817.png\",\n",
        "    \"/content/plots_nova_90d/nova_hist_90d_20250907-132817.svg\",\n",
        "    \"/content/plots_nova_90d/nova_bands_90d_20250907-132817.png\",\n",
        "    \"/content/plots_nova_90d/nova_bands_90d_20250907-132817.svg\",\n",
        "    \"/content/plots_nova_90d/nova_cdf_90d_20250907-132817.png\",\n",
        "    \"/content/plots_nova_90d/nova_cdf_90d_20250907-132817.svg\",\n",
        "]\n",
        "\n",
        "zip_path = \"/content/plots_nova_90d_selected.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for p in FILES:\n",
        "        if os.path.exists(p) and os.path.getsize(p) > 0:\n",
        "            zf.write(p, arcname=os.path.basename(p))\n",
        "            print(\"✔ added:\", p)\n",
        "        else:\n",
        "            print(\"⚠️  skipped (not found or empty):\", p)\n",
        "\n",
        "print(\"\\nCreated:\", zip_path)\n",
        "files.download(zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Xf-2PmDlcCa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}